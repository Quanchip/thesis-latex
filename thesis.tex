\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{graphicx, float}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage[skip=10pt plus1pt, indent=40pt]{parskip}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{times}
\usepackage[numbers]{natbib}
\usepackage{url}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{bookmark}
\usepackage{enumitem}
\usepackage{pgfgantt}
\usepackage{booktabs}
\usepackage{array, multirow}
\usepackage{amsmath}



\renewcommand{\cftchapfont}{\bfseries}
\renewcommand{\cftchappagefont}{\bfseries}
\renewcommand{\cftchappresnum}{Chapter }
\renewcommand{\cftchapaftersnum}{:}
\renewcommand{\cftchapnumwidth}{6em}

\titleformat
{\chapter} % command to format
[block] % shape: hang, display, block, frame 
{\bfseries\Large \centering}  % format of label + chapter title
{CHAPTER\ \thechapter:} % label "Chapter 1:"
{1.5ex} % separation label - chapter title
{} % code before

\renewcommand{\baselinestretch}{1.5} 

\graphicspath{{img/}}

\begin{document}

\thispagestyle{empty}
\begin{center}
    \textbf{VIETNAM NATIONAL UNIVERSITY OF HO CHI MINH CITY}\\[6pt]
    \textbf{INTERNATIONAL UNIVERSITY}\\[6pt]
    \textbf{SCHOOL OF COMPUTER SCIENCE AND ENGINEERING}\\[50pt]
\end{center}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{iu-logo.png}\\[50pt]
\end{figure}

\begin{center}
    \textbf{\huge {Anomaly Detection in HDFS Logs}}
    \textbf{\huge {Using Machine Learning Integrated With
            LLM-Based Mitigation}}\\[50pt]

    \textbf{By}\\
    \textbf{Nguyen Hoang Quan - ITITIU21291}\\[50pt]
    \textbf{The thesis submitted to School of Computer Science and Engineering
        in partial fulfillment of the requirements of the degree of Bachelor of Engineering
        of Information Technology}\\[70pt]

    \textbf{Ho Chi Minh, Viet Nam}\\
    \textbf{2025}
\end{center}

\newpage
\pagenumbering{roman}
\newgeometry{
    % Define the margin values:
    left=1.5in,
    right=1in,
    top=1.5in,
    bottom=1.5in,
    % Optional: Define the paper size if you haven't already
    % a4paper 
}


\begin{center}
    \textbf{\Large{Anomaly Detection In HDFS Logs Using Machine         Learning Integrated
            With LLM-Based Mitigation
        }}\\[80pt]

\end{center}

\begin{center}
    \textnormal{APPROVED BY: }


    \hspace{0.4\linewidth} \hrulefill \\ % Bắt đầu ở giữa (50% chiều rộng) và kéo dài sang phải
    \vspace{1.2cm} % Điều chỉnh dòng xuống nếu cần


    \hspace{0.4\linewidth} \hrulefill \\ % Bắt đầu ở giữa (50% chiều rộng) và kéo dài sang phải
    \vspace{1.2cm} % Điều chỉnh dòng xuống nếu cần


    \hspace{0.4\linewidth} \hrulefill \\ % Bắt đầu ở giữa (50% chiều rộng) và kéo dài sang phải
    \vspace{1.2cm} % Điều chỉnh dòng xuống nếu cần

    \hspace{0.4\linewidth} \hrulefill \\ % Bắt đầu ở giữa (50% chiều rộng) và kéo dài sang phải
    \vspace{1.2cm} % Điều chỉnh dòng xuống nếu cần

    \hspace{0.4\linewidth} \hrulefill \\ % Bắt đầu ở giữa (50% chiều rộng) và kéo dài sang phải
    \vspace{1.2cm} % Điều chỉnh dòng xuống nếu cần

    \hspace{0.5\linewidth}
    \textnormal{THESIS COMMITTEE}


\end{center}


\newpage

\vspace{-20pt}

\begin{center}
    \textbf{\large{ACKNOWLEDGEMENTS}}\\[20pt]
\end{center}

First and foremost, I would like to express my sincere gratitude to the School of Computer Science and Engineering for providing a supportive academic environment and the essential resources that made this thesis possible. \\[10pt]
I am deeply thankful to my advisor, Dr. Le Hai Duong, for his invaluable guidance, encouragement, and constructive feedback throughout the development of this thesis. His expertise and commitment has greatly enriched both my technical understanding and research direction. \\[10pt]
I would also like to extend my appreciation to all lecturers and staff members who have contributed to my academic journey with their instruction, support, and mentorship.
Special thanks go to my family and friends for their constant support, motivation, and understanding during the preparation of this work.
Lastly, I would like to acknowledge the developers and the researchers behind the resources for providing the foundation upon which this project was built. \\[10pt]
This thesis would not have been possible without the guidance, resources, and support from all of the above.


\newpage
\tableofcontents

\newpage
\listoftables

\newpage
\listoffigures

\newpage
\pagenumbering{arabic}
\newgeometry{
    left=1.5in,
    right=1in,
    top=1in,
    bottom=1.5in,
}

\addcontentsline{toc}{chapter}{ABSTRACT}

\begin{center}
    \textbf{\large{ABSTRACT}}\\[15pt]
\end{center}


Anomaly detection is essential for managing today’s large-scale distributed systems, where system logs are a key resource for identifying unusual behavior. Traditionally, system operators relied on manual inspection methods such as keyword searches and rule-based matching. However, due to the massive volume and complexity of modern system logs, manual approaches are no longer practical. To tackle this, many automated log-based anomaly detection methods have been proposed. Still, developers often struggle to choose a suitable method, as there hasn't been a clear comparison of these approaches.

In this research, I will propose a solution to addresses the fundamental challenge of automated log analysis. Specially, the research tackles three interconnected problems: (1) automated parsing of diverse log formats into structured templates, (2) anomaly detection in high-volume log streams, and (3) generation of contextual, actionable recommendations for identified issues.

Previous research has established some foundational approaches including the algorithms for log parsing and machine learning techniques for anomaly detection. However, existing solutions typically focus on individual components rather than providing end-to-end integration. Most academic implementations lack production-ready deployment architectures, user-friendly interfaces, and the integration of modern Large Language Models (LLMs) for intelligent recommendations—creating a significant gap between theoretical algorithms and practical deployment. Therefore, this research is important since it close the gap between academic log analysis algorithms and production-ready systems.

Furthermore, the research establishes an application that integrating emerging LLM capabilities into traditional system administration workflows, suggesting broader implications for AI-assisted practices. The findings indicate that intelligent automation of log analysis is not only technically feasible but can significantly enhance organizational capabilities in system reliability, security monitoring, and operational efficiency.

\restoregeometry
\newpage

\newgeometry{
    left=1.5in,
    right=1in,
    top=1in,
    bottom=1.5in,
}


\setcounter{page}{1}

\chapter{INTRODUCTION}
\section{Background of the study}


In modern software systems, log files serve as critical sources of information
for system monitoring, debugging, troubleshooting, and security analysis. As applications or systems scale and increase its complexity,
the volume of generated log data gets bigger exponentially. Therefore, traditional manual approaches
for log analysis like manually examine through log files using basic tools such as grep or text
editors, has become less efficient and more defective \cite{author2023}. As the result,
it is becoming more difficult to detect anomalies within large scale system.

Over the year, a lot of automated log-based methods have been introduced to help detecting
system anomalies. These approaches usually require raw log preprocessing techniques, feature extraction
and machine-learning-based algorithms for processing vast amounts of
unstructured log data efficiently, identifying potential issues before they
escalate into critical failures. Recent advancements in natural language processing
and large language models (LLMs) have also increased the potential for
intelligent log analysis systems. However, despite these development, traditional machine-learning techniques and LLM-based approaches have yet to be efficiently integrated
into a single, cohesive log analysis framework.

This study focuses on leveraging existing machine learning approaches where log parsing and anomaly
detected are used, and augmenting them with large language model to provide  actionable insights and helpful recommendations.


\section{Problem Statement}

Despite the important role of log analysis that play in making the system more secure and reliable,
several significant challenges still persist in current practices:

\begin{itemize}
    \item \textit{Limited interpretability.} In log anomaly detection,
          the ability to interpret model's outputs is important for people who
          work as system administrators or analysts to effectively action to
          the alerts. They need to understand which log entries may be responsible
          for the detected abnormality. Yet, many traditional approaches only
          provide basic classified prediction without any explanation. As a result,
          engineers still have to perform further manual root cause analysis
          which in large-scale and complex systems becomes an very time-consuming
          and heavy task.

    \item \textit{Poor adaptability.} Many existing methods rely on a predefined
          set of log event templates during feature extraction phase (This phase will use the set of log event templates generated by
          log parsing to create numerical features for machine learning models \cite{SARHAN2024205}).
          However, as applications scale up and expand  in term of feature,
          new and unseen log events will definitely appear. Therefore, adapting to these
          changes require retraining models from scratch which make the systems become less practical
          in dynamic environments.

    \item \textit{Poor adaptability.} Many current approaches are using static rules or models
          trained on old data which make them struggle to adapt to new log patterns or new unseen failures.
          This lack adaptability reduces the effectiveness of the methods and make it impractical in
          real-word scenarios.

\end{itemize}

\section{Objectives of the Study}

This study aim to perform the investigation on existing anomaly-detection
methodologies, compares the performance of different machine-learning
models, and develops a practical framework that integrates large language
models (LLMs) to automate and improve anomaly detection in real-world scenario.
At the end of the study, there are some main objectives that this study aim to
achieve:

\begin{enumerate}[label=(\arabic*)]
    \item Implementing an application for providing a better user interface in
          anomaly detection fields.

    \item Applying parsing tools and machine learning algorithms for log
          anomaly detection.

    \item Integrate large language models into the system to generate
          explanations and actionable mitigation recommendations.
\end{enumerate}


\section{Limitations of the Study}

Several limitations are still presented in this research. Firstly, the study focused only
on HDFS logs, so therefore the result may not be directly applicable for other types such
as application logs or network device logs. Second, the model evaluation was conducted on
pre-labeled dataset which may not fully address the challenges that related to real
production environments. Finally, scalability testing was not conducted on the system to
examine its performance under extensive user loads. The system is designed to handle
scalability in mind but the real validation was beyond the scope of this thesis.



\chapter{LITERATURE REVIEW}

\section{Introduction to system log}

System logs, also known as event logs or audit trails, are automatically generated by
a computer system (its operating system, services, applications) that record events, changes and error that
occur inside that system. These log files serve an important role in monitoring system
health, diagnose failures, detecting anomalies, and ensure system security and compliance \cite{STUDIAWAN20191}.
System logs usually follow a semi-structured format which consist of several fields that capture
both the context and content of the event.\\[10pt]


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{hdfs-log.png}
    \caption{An example of raw logs taken from HDFS dataset}
    \label{fig:hdfs-log} % The label for cross-referencing
\end{figure}

Figure 2.1 show a log snippet that was generated by HDFS DataNode, one of the core storage components in the Hadoop ecosystem
(a more detail explanation of this system will be provided below).
These log entries are created from the storage operations of the system such as
data writes, packet handling, and communication between DataNodes. A log entry typically follow a structure as follow:

\begin{itemize}
    \item \textit{Timestamp: 2016-10-02 12:24:52,337.} Indicates the exact moment when the system processed the event.

    \item \textit{Log level: INFO.} Shows that the entry reports normal operational activity

    \item \textit{Log message: org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace:...} Log messages provide a structured description of system behavior,
          capturing events, performance indicators. This is the most important part of a log entries and therefor become the
          foundation for monitoring, auditing and anomaly detection.

\end{itemize}

Given their important role in providing such essential information about the system, logs must be reliably collected,
centrally stored and appropriately maintained  to enable accurate monitoring, troubleshooting, and subsequent analytical processes.

\section{Traditional log analysis method}

Traditional log analysis have relied on two primary approaches (cite something): manual log inspection and rule-based pattern matching systems.
While these techniques are the foundation of the system administrators fields and troubleshooting, it were used during the time
when the system complexity and log volumes were much smaller than nowadays modern systems. Understanding the strengths and limitations of
these approaches will provide you a better context and insights of logs which will enhance your log analysis systems.


\subsection{Manual Log Inspection}

Manual log inspection is one of the earliest and most straightforward methods used by system
administrators to identify issues. In this approach, developers or administrators will
directly operate searching on raw log files using command-line tools such as \verb|grep| , \verb|tail|, \verb|less|,
to locate error logs or correlate timestamp of the incidents. While feasible for small log volumes, this method is very
time-consuming, highly error-prone and relies heavily on an operator’s familiarity with the system. Therefore, this approach
is no longer a feasible solution for detecting anomalies in large-scale systems which may produce millions of log entries
per hour. For instance, in 2013 the Alibaba Cloud system was reported to generate approximately 100 to
200 million log lines in one hour \citep{6410318}.

\subsection{Rule-Based Systems (Regex and Pattern Matching)}

To counter the limitations of manual log review, developers created rule-based system that used fixed rules, regular expressions (regex),
and predefined patterns to filter, parse, and identify specific events when a specification conditions are met.
For example, system operators can create rules to detect keywords such as "ERROR", "FAILED" or extract key information
from log entries. Rule-based methods offer some advantages: they are easy to implement and effective for detecting
obvious or recurring errors. Many traditional monitoring tools such as Nagios, Splunk (early versions)
\cite{splunk_field_extraction}, and Logstash \cite{logstash_grok},
they are heavily depend on popular pattern-matching techniques to filter, categorize, and index log data.

However, the effectiveness of rule-based approaches highly depend on the completeness and accuracy of the
hand-crafted predefined rules. It will become more defective in a constantly evolving environments where logs
format change frequently and new types of anomalies will appear which do not match the existing patterns. Maintaining
large sets of regex rules is also a labor-intensive action and require continuous manual updates on patterns which is
unsuitable for adaptive anomaly detection.



\section{Log parsing}

After log collection, raw logs are often semi-structured which contains constant parts and
variable parts and may vary among different entries. Log parsing process responsibility is
to transform those raw unstructured logs into structured format for downstream analysis.
Log parsing techniques will try to identified constant/static fields and dynamic fields.
The constant part are often log events, log template and the variables part are often
the fields that can change (e.g., IP address, thread name, job ID, message ID). For example
in Figure \ref{fig:parser}, a log entry is collected from Hadoop Distributed
File System (HDFS) dataset \cite{zhu2023loghub} "Received block-562725280853087685 of size 67108864
from 10.251.91.84" is parsed to log event
``Received block \textless{}*\textgreater{} of size \textless{}*\textgreater{} from \textless{}*\textgreater{}'',
where all changed parameters are captured
in ``\textless{}*\textgreater{}''. There are two common parsing techniques:
clustering-based (e.g., LogSig \cite{tang2011logsig}, LKE\cite{fu2009execution})
and heuristic-based (e.g., SLCT \cite{vaarandi2003data}, iPLoM \cite{makanju2009clustering}). In the former method, distance between logs is
calculate at the beginning, and use clustering techniques to group logs in to cluster. Event template
is generated at the end for each cluster. For the heuristic-based approaches, it count the appearance
of ech log entries and composed frequent words as event candidate. And finally, some are chosen
to become log events.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{parser.png}
    \caption{Example of Log Parsing}
    \label{fig:parser} % The label for cross-referencing
\end{figure}




\section{Machine learning and deep learning for log anomaly detection}

As system continuously scaled and logs volumes grew bigger, manual or rule-based approaches is
no longer available. As a result, researchers began to adopt machine learning to automate
anomaly detection. These methods are created to enable it to learn the structures, numerical representations,
and patterns of logs without depending on predefined rules or templates. These approaches are often categorized as
supervised or unsupervised learning.

\subsection{Supervised learning}

\begin{enumerate}[label=\alph*.] % Sets the label format to (a), (b), (c)...
    \item Support Vector Machine (SVM)

          Support Vector Machine is one of the fundamental methods that proposed
          by Vapnik \cite{vapnik1998statistical} which is used for binary classification.
          The basic idea of this method is as follow. For nonlinear separable data, we
          can transform it to higher-dimensional space where the data becomes linearly separable.
          This allows the classifier to construct an optimal separating hyperplane between normal
          and anomalous log patterns. In log detection context, anomaly logs will be treated as negatives
          and normal one will be served as positive examples. But using this method for detecting
          anomalies often often suffers from class imbalance, where the positive examples is outnumber
          the negatives. As the result, the classifier will overfit the anomalous logs, which make
          it become weak in detecting new occur, unseen logs. Figure \ref{fig:svm} shows principle
          behind using SVM to create an optimal boundary between normal and anomalous log database
          in a feature space.

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.7\textwidth]{svm.jpg}
              \caption{SVM classification}
              \label{fig:svm} % The label for cross-referencing
          \end{figure}

    \item Decision Tree (DS)

          Decision tree is one of the earliest machine learning approaches that apply to
          anomaly log detection and are defined by Quinlan \cite{quinlan2014c4} as "powerful
          and common tools for classification and prediction". This method recursively
          splitting data based on its features then create a tree-like structure where
          each leaf node represents a prediction class (normal or abnormal).
          When apply to log anomaly detection, we have to transform log entries into
          structured numerical features before feed it into decision tree algorithm.
          The applications of decision trees to log-based anomaly detection has been
          proposed in many research with promising results. He et al. \cite{7774521}
          did some experiments with anomaly logs in HDFS dataset using decision tree classifier and
          and prove the effectiveness of it. The approach not only achieve high accuracy
          but also provide interpretable rules that allows others to validate and refine
          them as they wish.
          In context of this research, decision tree served as a foundation machine
          learning algorithm for detecting anomalies in log data. The specific implementations
          and results of using this algorithm will be discussed more upon in subsequent sections.

          % \begin{enumerate}[label=\arabic*.]
          %     \item The first item in the list.
          %     \item The second item in the list.
          %     \item The third item in the list.
          % \end{enumerate}



    \item The final item summarizes the topic.
\end{enumerate}

\subsection{Unsupervised learning}

In many situation when anomaly data is not labeled or unavailable, the supervised
learning is thereby become impractical. As a result, unsupervised learning methods
have been adopted widely for log anomaly detection. These approaches aim to analyze
data without requiring labeled examples by discovering the characteristics of the
data itself. This section will examine some popular unsupervised approaches that are
currently applied to log analysis.

\begin{enumerate}[label=\alph*.] % Sets the label format to (a), (b), (c)...

    \item Isolation Forrest

          Isolation Forrest is a machine learning algorithm that was introduced by Liu et al
          \cite{liu2012isolation} in 2012. His core principle of isolation forrest is that
          anomalies are more accessible to isolate than anomalous points \cite{Longberg2024}.
          The algorithms will construct forrest of random binary trees to separate each
          data point. Each tree us built by randomly selecting a feature from the dataset
          and a split value within the feature's range, then repetitive split the
          data until it reach the maximum depth or the instance is isolated.

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{isolation-forrest.png}
              \caption{Isolation Forest}
              \label{fig:isolation} % The label for cross-referencing
          \end{figure}

          Figure \ref{fig:isolation} demonstrates the isolation capabilities of Isolation Forest algorithm.
          In the left, the data point is separated with only one split, indicating a high anomaly score.
          On the other hand, data point on the right requires more splits,  suggesting it’s a
          nominal data point
    \item Principal Component Analysis (PCA)

          Principal Component Analysis is a common technique that widely used for dimensionality reduction
          \footnote{Dimensional reduction is the process of reducing the number of input variables
              (features) in a dataset keeping as much essential information as possible.}
          in a dataset. By doing so, PCA algorithm helps simplifying complex dataset, reducing
          the redundancy among features, and highlighting the most important patterns in the data
          \cite{jolliffe1990principal}. In anomaly detection, it first converts each log sequence
          into event count vector by counting how many time each type of log event appear in the
          sequence. Next, the PCA algorithm \cite{lee2006application,abdi2010principal}
          will capture the main patterns of normal log behavior then project those vectors to
          a learned space (also known as normal space). However, when a log sequence contains
          anomaly logs such as missing events or unusual log messages, PCA algorithm will mark
          it as abnormal behavior by calculating the Square Prediction Error (SPE) \cite{jackson1979control}
          , which measures the deviation from the learned normal patterns.




\end{enumerate}


% Parsing with LLMs instead of templates:

% % https://zeyang919.github.io/paper/LLMParser.pdf?utm_source=chatgpt.com

% Explainability + anomaly detection:

\chapter{METHODOLOGIES}


\section{Overview}

This chapter describes the methodology that used to design, implement, and evaluate the
proposed log anomaly detection framework. The methodology follows a pipeline that begins with
data collection and preprocessing, then extract the features and using machine learning techniques
to detect anomalies. Large language models (LLMS) are also integrated to address the limitations
of the traditional approaches, giving more insights and contextual understanding of detected
anomalies. This chapter is organized to present each stage of a framework, from system architecture
and how data is prepared to model Implementation and evaluation.

\subsection{Research Approach}
In this study, I adopt a design-based experimental research approach that combines traditional
machine learning methods with large language model techniques. This approach will focus on
constructing a practical, user-friendly and production-ready log anomaly detection framework
by integrating proven traditional methods with LLM-based reasoning to addressing both the technical
and operational challenges of large-scale log analysis.

The methodology combines well-established components such as the Drain log parsing algorithms to
constructs structured log templates from raw log messages, Decision Tree or Isolation Forrest
machine learning models are compared to find the most effective one and implemented it as
detection instruments.

For data collection, this study relies primarily on secondary collection methods which uses public
and available log datasets like HDFS system logs or BGL logs dataset. There will be no primary data involving human
participants are collected. The data collection process includes log ingestion, parsing, feature extraction,
anomaly labeling (already prepared by experts), and preparation for model training and testing.

This study use a mixed-method of quantitative and qualitative analysis approaches. Quantitative analysis is utilized to evaluate the performance of detection models
using real statistics (e.g., detection rates and error rates) and comparison between models.
Furthermore, qualitative analysis is applied to the outputs that is generated by LLMs, focusing
on their ability to provide root cause explanations and mitigation suggestions.

Ethics are also considered in this study which relate to data usage and research integrity.
All datasets that I used are publicly available so it ensures no personal information is
exposed. Proper citation and acknowledgment of datasets, tools, and prior research are also maintained.
Additionally, this study is responsible for the LLM-generated outputs by put it as only suggestions
rather than automated direct action, and thereby preventing unintended operational impact.

\subsection{System Pipeline Overview}
The proposed framework follows a pipeline from transform raw systems logs into structured one and useful
recommendations through a sequence of stages. Figure \ref{fig:pipeline} illustrates the overall pipeline.

\begin{enumerate}[label=(\arabic*)]
    \item The pipeline begins with log preprocessing, where raw logs are parsed using the Drain algorithm. This
          step will convert raw and unstructured log into sequence of structured events and extract event templates.

    \item Structured log sequences will be the input for the processing step, where it will be transformed to
          numerical representations and then fed to machine learning models for anomaly detection.

    \item The final stage integrates large language models (LLMs) to enhance system interpretability. LLMs component
          will perform semantic analysis based on detected anomalies and generate human-readable explanations about
          root causes and useful suggestions.
\end{enumerate}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{pipeline.png}
    \caption{System pipeline}
    \label{fig:pipeline}
    % \label{fig:my_image} % The label for cross-referencing
\end{figure}


\section{System Architecture and Design}
This section presents the system architecture and design of the proposed log anomaly detection framework.
We will focus on the structural composition of the system and also show their responsibilities, and interactions.
The architecture is designed to make the process of large-scale log data more efficiently while enabling to
integrate with traditional anomaly detection techniques and large language models (LLMs).

\subsection{Architectural Overview and Design Principles}

The proposed log analysis system follows a simple architecture but highlight the
practicality and ease of integration to support real-world problems. The systems is desgined around some
key principles:


\begin{enumerate}[label=\arabic*.]
    \item Asynchronous Processing

          The architecture apply asynchronous task processing to handle computationally heavy workload without
          affect user interface. Therefore the system can effective process large log files while also maintain
          good user interaction.

    \item Modular Service Design

          Each component in the system is designed as an independent module with clear responsibilities to maintain or test
          more easily, and also increase the ability of horizontally scaling of individual components.

    \item Data Layer Separation

          The system create a clear separation between metadata management and object storage that using
          MinIO for large-scale objects and PostgreSQL for logs structured metadata. This design optimizes
          the performance by addressing the data access pattern problem where PostgreSQL is suitable for
          structured queries and indexing whereas MinIO offer high throughput and cost efficiency for
          actual log storage.

    \item Multi-Stage Processing Pipeline

          In my log analysis system, I implemented a multi-stage pipeline which consists of parsing, anomaly detection,
          and intelligent analysis phases. This allows me to test and integrate different algorithms and models
          that is the most suitable for the system without disrupting the entire workflow.

\end{enumerate}

\subsection{High-Level System Architecture}

The system uses a layered, distributed architecture which is designed to support efficient log ingestion and
anomaly detection. Each layer is responsible for a distinct functionality ensuring the ease for development
and maintenance. The layered architecture of the system is captured in Figure , and describes in detail
as follows.

\begin{figure}[H]
    \centering
    \hspace*{-2cm}
    \includegraphics[width=1.3 \textwidth]{architecture.png}
    \caption{System architecture}
    \label{fig:architecture}
\end{figure}

\begin{enumerate}[label=\arabic*.]

    \item Presentation layer

          This layer provides a user-friendly interface between end users and the log analysis system. It is a
          single-page application (SPA) that built on React framework, allowing users to upload
          log files, view processing status and final analyzed results. The system also develop
          authentication through JWT-based mechanism and role-based accesses for better security.

    \item API Gateway Layer

          The API gateway layer is responsible for create a communication between the frontend and backend.
          This layer expose REST API endpoints which will handle request routing, input validation and give back the
          responses by using FastAPI technologies.

    \item Service Layer

          The service layer hold the core business logic of the system. These services
          include authentication, handling log files, and managing model detection logic.
          and LLMs analysis.
          By separating functions into discrete services, the architecture allows each service to
          work independently and easy to change as you wish.

    \item Data Processing Layer

          Data processing is one of the most important steps in the entire workflow, so this layer
          is built very carefully. Raw log messages are parsed into structured one and extract event
          templates using Drain algorithm. Then using the structured logs to ingest to machine learning models
          and save as serialized file for real-time inference. Anomalies is then analyzed by LLMs
          to generate explanations about how it mark as anomaly and mitigation suggestions.

    \item Storage Layer

          The storage layer is designed to handle how log data will be stored and accessed effectively.
          MinIO is used to store actual raw log files that users upload to the server.
          I utilized this technology because its object storage allows storing and access large volume
          of data with high speed and low cost. Whereas PostgreSQL is chosen to manage structured metadata
          due to its ability to create complex structured queries. An in-memory cache (Redis) is also
          employed in this system to support fast access to data such as task queues.


    \item Task Management Layer

          To support asynchronous processing as I mentioned in section 3.2.1, the system also create a
          layer for task management. Each step such as parsing, anomaly detection, and LLM inference will
          be define as tasks and put to message queues. Celery workers will take tasks from queue and execute
          in the background, thereby parallel execution is made possible.

\end{enumerate}



\section{Dataset preparation}

The HDFS log data set is the most frequently used data set for anomaly detection methods and
thus also the main focus point of this study. The logs were collected from the Hadoop Distributed
File System (HDFS), which runs on the Amazon EC2 platform. This system is designed to run on
commodity hardware and allows users to store and process large files. Each log event have one or more
block identifiers which will enable grouping logs into sequence of events. And in fact, the sample
logs snippet show in Figure \ref{fig:hdfs-log} are taken from HDFS daWta set. The core idea behind
the anomaly detection in this dataset is that some data blocks is fail to be processed by the
system. When such failures happen, the blocks generate log events that differ from normal processing behavior.
Therefore, the entire sequence should be detected as anomalous \cite{xu2009detecting}.

The data was originally collected by Xu et al. from a production hadoop cluster comprising
more than 200 nodes. The dataset was labeled by domain experts to distinguish normal
and anomalous execution flows. In this study the HDFS dataset version is taken from project
Loghub \cite{zhu2023loghub}. The total lines of log messages in this dataset version is
11,175,629 but a close inspection of the dataset shows that approximately 22,000 lines are missing in comparison to the original data set for
unknown reasons. However, the remaining data still provides a sufficient sample for
anomaly detection analysis and techniques evaluation.


\section{Log Parsing Methodology}

In this section, I briefly introduce to Drain the main parsing techniques used in this study which is
a fixed depth tree-based method. The core principle of this algorithm is to organize log messages into
a parse tree where each node represents a token position in the log messages \cite{he2017drain}.
Drain parser first tokenize the log message typically using whitespace as delimiter. For example,
log message like "Received block-562725 from node 10.0.0.1" is split into
    ["Receive", "block", "blk\_562725", "from", "node", "10.0.0.1"] and these tokens are used to
traverse through layer of a parse tree. The parse tree is illustrated in Figure \ref{fig:parse-tree}.
At the first layer under the root node, the tree branches are represented number of tokens in
log messages. Messages with the same length are routed to the same subtree. At the next
layer , Drain use specific tokens to guide the route. If token have words like ``Sends'' or
``Receive'', it is use as branch key. If token is identified as variables part, it will be
replace with symbol *.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{parse-tree.png}
    \caption{Structure of Parse Tree in Drain (depth = 3)}
    \label{fig:parse-tree}
    % \label{fig:my_image} % The label for cross-referencing
\end{figure}


\section{Feature Extraction / Representation}

The main purpose of this step is to extract features that is valuable that can
be utilized by anomaly detection models. The input of this step is
structured log events generated during the log parsing then the output is
event count matrix that represent characteristics of log sequences.
The log must be organized in log sequences in order to construct features.
This grouping process can be done through windowing techniques.
In this project, three windowing techniques are considered.

\begin{enumerate}[label=\arabic*.]
    \item Fixed Window

          Fixed windows groups logs based on timestamp using predefined tim duration.
          All log event that occurred in this duration are grouped in to log sequence.
          This approach is simple but mat mix unrelated log entry if it occur in the
          same time window.

    \item Sliding Window

          Sliding window is the enhance version of fixed window by adding a step size to
          window size.  In general, step size is smaller than window size, therefore
          overlapping windows is occur.

    \item Session Window

          Session window is based on identifiers rather than timestamp compare to two
          other windowing types. This method is practically suitable for distributed
          system where every log contains an identifiers. As a result, logs with
          the same identifiers are are grouped into a single session window.
          In this project, session-based windowing is primarily used for the HDFS
          dataset.
\end{enumerate}

Once the log sequences are created, feature vectors are generated by counting the occurrence
of each log event in a sequence. Each log sequence is represented as an event count vector.

\section{Anomaly Detection Models}

To evaluate and select the most suitable approach for the anomaly detection framework,
multiple machine learning models were trained and compared. Supervised machine learning
algorithms that used are Decision Tree and Support Vector Machine (SVM), while unsupervised
methods are Log Clustering, PCA, Invariants Mining, and Isolation Forest.
Each model was trained using the same training dataset to ensure a fair comparison.
Performance was evaluated on a test set using precision, recall, and F1-score.

% \section{LLM Integration Method}

% In this project, Large Language Models (LLMs) is not used for anomaly detection but
% to enhance the interpretability of detected anomalies. The LLMs is mainly used to
% explore the logs that are marked anomalies then generate explanations and
% mitigation recommendations.





\chapter{IMPLEMENTATIONS}
\section{Implementation Overview}

This chapter will present the implementation of the proposed log anomaly detection application. Each
components that are described in chapter 3 will be transformed to real functional code and integrate
in to the system. The scope of this chapter is focus on implementation details including
configuration, library used, API designs and technologies that are used to build
end-to-end system.


\section{Implementation timeline}

The system implementation is followed a structured timeline that consists of several
phases and each phase will focus on a specific tasks and will be described as follow:



\begin{itemize}
    \item \textit{Phase 1 (Week 1)}

          This initial phase focused on prepare and research background knowledge for the project. Some
          major activities are:

          \begin{itemize}

              \item Creating a literature review about log parsing, anomaly detection techniques,
                    and LLMs for log analysis.

              \item Understanding about HDFS dataset, how it was generated, and its structure.

              \item Defining system requirements and architecture.

          \end{itemize}

    \item \textit{Phase 2 (Week 2): Data Preparation}

          In this phase, raw actual log will be collected, and prepared for use in subsequent steps.

          \begin{itemize}

              \item Downloading raw HDFS dataset from LogHub repository and validating data.

              \item Using a small sample in the data set to understand the event structure.

              \item Set up a directories just for data storage purposes.


          \end{itemize}

    \item \textit{Phase 3 (Week 3-5): Backend Infrastructure Setup}

          The fundamental backend environment was set up carefully for later implementation:

          \begin{itemize}

              \item Setting up the FastAPI project structure, base routers, and middleware.

              \item Configuring PostgreSQL database, Redis, and MinIO containers using Docker.

              \item Designing database schema.

          \end{itemize}


    \item \textit{Phase 4 (Week 6-7): Log Parsing module development}

          This phase mainly focused on implementing a wrapper for Drain parser in order to integrate into
          the system, which includes some major steps:

          \begin{itemize}

              \item Developing a Python script to wrap the original Drain algorithm.

              \item Creating Celery tasks for asynchronous parsing.

              \item Designing storage solution for parsed logs (MinIO + PostgreSQL)

          \end{itemize}


    \item \textit{Phase 5 (Week 8-9): Log Parsing module development}

          Some popular ML anomaly detection models were utilized and evaluated in this phase:

          \begin{itemize}

              \item Extracting event count vectors and features.

              \item Training and validating models using parsed event sequences.

              \item Storing trained models for reusing in the processing pipeline.

          \end{itemize}


    \item \textit{Phase 6 (Week 10-11): LLM Integration}

          This phase focused on implementing LLM for more understanding of anomalies:

          \begin{itemize}

              \item Developing the LLM service module for interacting with the model API.

              \item Structuring a prompt templates for explanation, and mitigation suggestions.

              \item Integrating into the pipeline as the final stage.

          \end{itemize}

    \item \textit{Phase 7 (Week 12-13): Frontend Development}

          A functional web interface is constructed and delivered in this phase using React technology:

          \begin{itemize}

              \item Developing the LLM service module for interacting with the model API.

              \item Structuring a prompt templates for explanation, and mitigation suggestions.

              \item Integrating into the pipeline as the final stage.

          \end{itemize}

    \item \textit{Phase 8 (Week 14): Testing, Optimization, and Debugging}

          This phase performs several tests and optimization to ensure product stability:

          \begin{itemize}

              \item Optimizing database queries and model loading.

              \item Refining UI responsiveness and user experience flow.

              \item Running test with a full datasets to ensure all stages work smoothly.

          \end{itemize}


    \item \textit{Phase 9 (Week 15-16): Documentation and Thesis writing}

          The final phase will consolidate all the work and prepare the system for thesis writing:

          \begin{itemize}

              \item Documenting code structure, APIs, preprocessing steps, and models.

              \item Preparing visualizations, diagrams, and comparison tables.

              \item Create overall structure for thesis and complete the final work.
          \end{itemize}

\end{itemize}



\section{Backend Implementation}

The backend of the log analysis system was developed using FastAPI - a popular Python framework
, due to its ease of use and high performance.
The implementation follows the layered architecture that introduced in Chapter 3 and therefore separate the
entire system into modular routers and components in order to maintain and scale more easily.

\subsection{FastAPI Application Architecture}

\begin{enumerate}[label=\alph*.]

    \item Application Initialization

          The main application entry point is defined in `src/main.py` as shown in
          Figure \ref{fig:main.py}, where all routers and middlewares are registered.
          The main application is also responsible for initializing CORS middleware and create all database
          tables that defined in SQLAlchemy models whenever the application start. This setup system features like Auth, Jobs
          Logs, to be managed by it own route files.


          \begin{figure}[H]
              \centering
              \includegraphics[width=0.6\textwidth]{main-py.png}
              \caption{main.py file}
              \label{fig:main.py}

          \end{figure}


    \item Router Organization

          \begin{table}[h]
              \centering
              % Sử dụng '|' để tạo đường kẻ dọc và '>{\ttfamily}l' cho phông chữ code
              \begin{tabular}{|l|l|}
                  \hline
                  \textbf{Router} & \textbf{Responsibilities}                      \\
                  \hline
                  auth\_router    & Login, registration, token generation          \\
                  \hline
                  log\_router     & File upload, metadata retrieval                \\
                  \hline
                  job\_router     & Job creation, status queries, result retrieval \\
                  \hline
                  ml\_router      & Model inference endpoints                      \\
                  \hline
                  llm\_router     & Explanation and mitigation generation          \\
                  \hline
              \end{tabular}
              \caption{Router Responsibilities}
              \label{tab:module_responsibilities_final}
          \end{table}

          Each domain in the system is separated to different router to maintain the ability to
          scale, be readable when number of features and endpoint grows. The table \ref{tab:module_responsibilities_final}
          gives information about the total router that are implemented in the systems and their
          responsibilities. Routers itself will only communicate with service layers to mainin a well
          and clear application architecture.

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{router.png}
              \caption{Example of a router configuration pattern}
              \label{fig:router.py}
          \end{figure}


          Figure \ref{fig:router.py} captures a code snippet that represent a pattern used for
          implementing router configuration. Each router will contain several endpoints that
          directly related to that functional service layer. For example, the authentication
          router includes endpoints for user creation, get login users or send access tokens.
          This approach allows developers to organize the application better and avoid as much as
          confusion in source code.




\end{enumerate}
% - Code snippets from src/main.py
% - Router organization
% - Dependency injection
% - Database session managementPhase 


\subsection{Database Layer Implementation}
fSupport Vector Machine (SVM)
The database layer was built based on PostgreSQL database with SQLAlchemy ORM which allows developers
to interact with relational databases using Python classes and objects instead of using raw SQL queries.
All entities in the system such as users, logs, jobs and analysis are represented as ORM models ann then
mapped to relational database tables.

\begin{enumerate}[label=\alph*.]

    \item Database schema

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{schema.png}
              \caption{System database schema}
              \label{fig:database_schema}
          \end{figure}

          Figure \ref{fig:database_schema} depicts 4 core tables in the anomaly detection system, including \texttt{users},
          \texttt{log\_files}, \texttt{processing\_jobs} and \texttt{analysis\_results}. Each table serves a distinct pupose
          and are described as follow:


          \begin{itemize}

              \item Table users: This table will manage users information such as user name, password,
                    email adress and their role in the system.

              \item Table log\_files: This table stores metadata of a log file that uploaded by users,
                    and there will be a link to its actual storage location in MinIO.

              \item Table processing\_jobs: This table is primarily used for storing jobs information that
                    create by the system to track the Celery background tasks.

              \item Table analysis\_results: ML anomaly detection results for eacg analyzed log filed are
                    stored in this table with some information such as number of sequences, anomaly and normal sequences
                    count


          \end{itemize}

          The database schema also implement logical relationships between entities. Each user can upload
          multiple log files and create multiple processing jobs. The log file entity is a central reference
          point to connect both processing jobs and analysis results. This implement means each log file
          can be associated with multiple processing jobs, which happen when reprocessing is required due to
          any failures in the pipeline. In addition, each log file can also link to multiple analysis results
          to support the LLM integration stage.


    \item Model implementation
          \begin{figure}[H]
              \centering
              \includegraphics[width=0.86\textwidth]{data-model.png}
              \caption{System database schema}
              \label{fig:data-model}
          \end{figure}

          In this project SQLAlchemy ORM (Object Relational Mapping) is used to create
          a clean abstraction over a complex relational database. There are will be four
          model configuration file for four entities coordinatly. Each file is defined how
          information of a enity is store and connected within our database. Figure \ref{fig:data-model}
          shows how a model (Users model) is initialize in the project. The Users entity stores
          authentication information with server key attributes such as id which is identified
          for each user, username and password to log into the system. This file also so handle
          relationship with other entities, in this case, Users model have a one-to-many relationship
          with the LogFile entity. Each user can upload multiple log files, and each log file
          link to its owner via a foreign key.
\end{enumerate}

% fasdfasd
% - model diagram
% - detail

\subsection{API Endpoints Implementation}

Table \ref{fig:endpoints} described all the endpoints that are exposed by the backend of
the system that cover authentication, log management, job processing, and LLM results generation.
Each endpoint is responsible for a specific functions in the entire processing pipeline.

\begin{table}[H]
    \centering
    \includegraphics[width=1\textwidth]{api1.drawio.png}
    \caption{API endpoints}
    \label{fig:endpoints}
\end{table}
\vspace{20pt}

\textbf{\large{Endpoints implementation detail:}}

This section will provide detail of technical implementation of some important API endpoints.

\begin{enumerate}[label=\arabic*.]

    \item POST \verb|/api/logs/upload|

          This is the core endpoint that handles the file upload process in the system workflow.
          It exposes a interface for users to upload their log file, store them in object
          storage system, register a record in the dataset and finally trigger an
          asynchronous processing jobs for further analysis. Detail code implement is
          captured in Figure \ref{fig:Upload-endpoints} and described as follow.

          The endpoint is implemented as HTTP POST route under path /api/logs/upload.
          It is defined using FastAPI's router and use asynchronous mechanism to
          handle request in order to ensure non-blocking I/O operations. When the backend
          receive a request from this endpoint, it first initialize a \texttt{LogService}
          instance which is a service layer that hold the business logic. The uploaded
          file is then validated for allowed extension. If the file extension does not match
          the \texttt{.log} extension, the method will immediately return HTTPException with
          status code 400 which is indicated a bad request error. The endpoint is continue
          to operate \texttt{LogService.save\_log\_file()} method which is responsible for
          uploading valid log files to MinIO and save its metadata to the database.
          Finally, the method return a structured results of several information.

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.9\textwidth]{upload-endpoint.png}
              \caption{Logs upload endpoints}
              \label{fig:Upload-endpoints}
          \end{figure}

    \item POST \verb|/api/auth/token|


          Figure \ref{fig:token-endpoints} describes a simple implementation of token
          processing endpoint.
          This endpoint is responsible for handling JSON Web Tokens (JWTs) to verify
          user within the system. Once the backend receive request, \texttt{UserService}
          will perform user finding, password verification and generate token.
          When the authentication is successfully done, the service will create JWT
          which contains user's information and attach it to request header when
          accessing protected resources.
          \begin{figure}[H]
              \centering
              \includegraphics[width=0.8\textwidth]{token-api.png}
              \caption{Token endpoints}
              \label{fig:token-endpoints}
          \end{figure}

\end{enumerate}




\section{Log Processing Pipeline}

This section provides detail implementations of multi-stage log processing pipeline
within the system. This pipeline is responsible for transform raw logs into
structured data and perform anomaly detection with machine learning algorithm.

\subsection{File Upload and Storage}

\begin{enumerate}[label=\alph*.]

    \item MinIO Client Integration

          MinIO was selected as the object storage solution in this project to store raw log data
          due to its high performance and suitable for containerized deployments. The
          system implements a Python MinIOClient wrapper class that abstracts from
          the MinIO Python SDK. Figure \ref{fig:minio} captures the
          code snippet of MinIOClient class from the system.

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.8\textwidth]{minio.png}
              \caption{minio\_client.py file}
              \label{fig:minio}
          \end{figure}

          When initializing this class, the client creates a connection using predefined
          access credentials and also verifies bucket with \texttt{\_ensure\_bucket\_exists()}
          method. This ensure the target bucket exist and automatically create it if
          necessary. The application also implement singleton pattern for MinIO client, where
          only one instance of \texttt{MinIOClient} is created and shared across the entire
          backend. This design allows system to maintain consistent connection with MinIO that helps
          uploading and storing operations faster.

    \item File Upload Implementation

          File upload mechanism is implemented inside \texttt{upload\_file()} method and are captured
          in Figure \ref{fig:file-upload}.
          The method first wrap log file data in \texttt{io.BytesIO} to transform data to
          file-like interface to that required by MinIO SDK. The the method perform
          an upload to existing MinIO bucket and a return a response with useful
          information when the upload is done.

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{upload-file.png}
              \caption{File upload method}
              \label{fig:file-upload}
          \end{figure}

    \item Unique Filename Generation

          To ensure files that uploaded to MinIO have the unique object name, the system develop
          a mechanism to create unique name for each object by combining timestamp, random
          characters and its original filename. As a result, each log file save to MinIO
          bucket will have result path as logs/{user\_id}/{timestamp}\_{uuid}\_{filename}.log.
          This strategy ensure the object name collision probability is extreme low and
          therefore reduce the error rate within the system. In addition, objects is store in
          separate user-scope directories which support better audition and efficient lookup.

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{minio-edit.png}
              \caption{Objects stored in MinIO}
              \label{fig:minio-name }
          \end{figure}
\end{enumerate}

% - MinIO client integration
% - File validation logic
% - Unique filename generation

\subsection{Asynchronous Processing with Celery}


\begin{enumerate}[label=\alph*.]

    \item Introduction to distributed task processing

          Distributed task processing has become increasingly important in modern system because
          this technology allows all tasks are distributed efficiently across multiple nodes.
          This approach enhances system scalability, performance, and enable extensive computation
          can be efficiently execute. Several tools and framework have been proposed to handle
          distributed task execution such as Apache Hadoop or Apache Spark. However, in this project,
          Celery is use to handle distributed task processing which is a framework that built on top
          of the Advanced Message Queuing Protocol (AMQP) paradigm. For the context of log anomaly
          detection, for example, parsing a 50MB HDFS log file with traditional subsequent methods
          require 30-100 seconds to finish, this means the entire system have to wait until the
          process complete. This issue exceed typical HTTP timeout thresholds, therefor degrading
          user experience and increase the risk of resource exhaustion. With Celery, instead of waiting for the entire processing
          to finish, the API only submits its task to a queue and immediately returns control to the user.
          Celery workers then fetch tasks from the message broker, execute them in the background,
          and update the job status in the database.

          % \begin{figure}[H]
          %       \centering
          %       \includegraphics[width=1\textwidth]{minio-edit.png}
          %       \caption{Objects stored in MinIO}
          %       \label{fig:minio-name }
          %   \end{figure}

    \item Celery Implementation and Configuration

          This system employs Celery as asynchronous task processing solution and also integrate with
          Redis as its message broker. This approach is provides a easier and more lightweight
          solution for task distribution without operational overhead of another message broker
          is RabbitMQ.

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.85\textwidth]{celery-config.png}
              \caption{Celery application configuration}
              \label{fig:celery-config}
          \end{figure}

          Figure \ref{fig:celery-config} presents a simple configuration to established
          Celery application and the connection to Redis message broker which is also act
          as a place to store task results. The architecture follows a standard
          producer-consumer model: when a use upload a log file through REST API,
          our FastAPI backend (producer) stores file in MinIO and then publishing a message(task)
          to Redis queue with an unique ID. Celery workers running in background, continuously
          poll the queue for pending task. When receiving a task message, a worker executes the
          processing logic, updates job status in the database,
          and optionally publishes results back to Redis.

    \item Task Implementation

          \begin{enumerate}[label=\arabic*.]

              \item Log Processing Task

                    The system implements a two-stage processing pipeline through chained Celery task.
                    The first task is \texttt{preprocess\_task} which is used for log parsing and
                    transform to structured csv file. The detail steps will be described as follow.
                    The Celery worker first retrieve the log file from MinIO object storage and decode
                    UTF8 content, then invoke the Drain parser service for template extraction.
                    The parser result is then transform into CSV format to provide human-readable
                    format and serve as a input to machine learning task. The structured CSV is
                    uploaded to other MinIO bucket and save the bucket information to database for
                    retrieving during ML task. Finally, it enqueue the next \texttt{ml\_analysis\_task}
                    to the message queue, this allows the current worker to complete and accept
                    new task while letting other workers to process ML task. All the implements as code
                    is show at Figure \ref{fig:log-task}.

                    \begin{figure}[H]
                        \centering
                        \includegraphics[width=1\textwidth]{log-task.png}
                        \caption{Log process task implementation}
                        \label{fig:log-task}
                    \end{figure}

              \item Machine Learning Analysis Task

                    This task is responsible for the second stage of the task chain, where it consume CSV
                    input, perform machine learning detection and present output. At the beginning of the
                    task, worker retrieve structured log CSV file from MinIO, the pre-trained model is
                    then invoked for prediction. After the prediction successfully done, all the
                    important information such as total logs, anomaly count, and prediction log
                    sequences are stored in to database. Only after the analysis task complete, the
                    entire job will be marked as COMPLETE.
          \end{enumerate}


\end{enumerate}



% - Celery configuration
% - DatabaseTask base class
% - Task retry logic
% - Job status updates

\subsection{Drain Parser Integration}


The system encapsulates the Drain parser algorithm that introduced in previous chapter within a
custom python class. The approach wrap the entire original complex Drain algorithm into a clean interface
for system to interact with. At the beginning the wrapper defines a format presets for
common log types. In the current implementation, the HDFS log format is defined using a fixed template that explicitly
separates timestamps, process identifiers, log levels, components, and message content.
This structured definition ensures consistent field extraction and reduces ambiguity during parsing.
When user invoke the the parser service with a specific
log format, these optimized settings wil be applied automatically. In Figure \ref{fig:drain},
the implementation will provide a constructor for \texttt{DrainParserService} with several parameters
such as similarity threshold (st), parse tree depth (depth), and maximum children per node (max\_child).
These parameters allow control over parsing behavior which will enable user to customize as
they wish. Overall, this approach provides an easy solution and reuseable interface for system
to integrate complex algorithm in anywhere.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{drain-imple.png}
    \caption{Drain algorithm wrapper implementation}
    \label{fig:drain}
\end{figure}



% - Parser service wrapper
% - Temporary file handling
% - Template extraction
% - Output structuring

\subsection{Feature Extraction}

The next step in the data preprocessing pipeline is feature extraction which
transform Drain-parsed data in to numerical feature vectors. This sections
presents the implementation of integrating Loglizer library's \texttt{FeatureExtractor}
class to convert event sequences into  fixed-dimensional count matrices
suitable for machine learning classification. In the proposed system, feature
extraction is applied at two distinct phases: (1) During training phase, where
model learned and serialized for reuse, (2) during online inference, where the same
transformation logic is applied to user lof data.

\begin{enumerate}[label=\alph*.]

    \item Training Phase

          Figure \ref{fig:feature} shows the configuration for feature extraction during the offline training
          process. Parse log is first loaded using session-based windowing strategy. This
          strategy group structured log events by session identifiers and will mark
          anomaly label at sequence level not entry level. Once the
          sequences are prepared, \texttt{FeatureExtractor} class is invoked to convert
          event sequences into numerical feature vectors that required by machine learning
          algorithms. This transform is based of event frequency statistics, which mean that
          whe count the occurrence the number of single log event to form the event
          count vector. Finally every count vector are formed an event count matrix $X$,
          where entry $X_{i,j}$ presents many times the event $j$ occurred in the $i$-th log sequence
          \cite{7774521}.

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{feature-extract.png}
              \caption{Feature extraction implementation}
              \label{fig:feature}
          \end{figure}

    \item Inference Phase

          In the inference phase, feature extraction is executed inside Celery background task.
          Parsed log sequences generate from new logs are transform using pre-trained feature extractor.
          This ensures that the same event vocabulary and weighting scheme learned during training
          are are consistently applied. This approach helps system avoids feature drift and maintains
          reliable outcomes.


\end{enumerate}


% - Event counting
% - Statistical features
% - CSV conversion

\subsection{Machine Learning Implementation for Anomaly Detection}

This section will evaluate multiple machine learning models for logs anomaly detection
to find the suitable approach that have the most suitable in term of performance and
system efficiency for the project. After log parsing and event sequences construction,
Several machine learning models were trained and evaluated on the same dataset
to ensure fair comparison. The base code for machine learning algorithms were provided
by Loglizer project \cite{7774521} as mentioned earlier in this study. Each algorithm is
training with default parameters to establish baseline performance before hyperparameter
optimization. The training pattern followed a consistent flow across all models as showing
in Figure \ref{fig:model}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{model-train.png}
    \caption{Standard training workflow}
    \label{fig:model}
\end{figure}




Finally, anomaly detection results are stored in the database for further
analysis and retrieval.

% - Model training code
% - Feature preparation and how ml predict anomaly
% - Prediction result
% - Comparison betweeb models and why choose Decision tree
% - Result storage

\section{LLM Integration}

Instead of stopping at machine learning detection phase of some previous study, my system
extends the ability to detect anomaly with Large Language Model (LLM). This approach
will provide human-readable explanations and recommendations for marked anomalies.



\subsection{API client setup and Context preparation}

\begin{enumerate}[label=\arabic*.]

    \item API client setup

          In this study, Groq's cloud-based inference platform is used to provides free hosting
          Meta's LLaMA 3.3 model. This approach will avoid the computation overhead and the
          complexity of the system. Figure \ref{fig:llm} create a API connection through HTTP client.
          The service configuration includes 3 keys components: (1) API key that used to
          connect with Groq service, (2) Groq API endpoint URL which specifically targeting
          its Chat Completion service, and finally (3) the choice of LLaMA 3.3 model that
          responsible for generating responses. This model is selected because its
          strong balance between the capability for reasoning and contextual understanding
          due to its large parameter size (70 billions).

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{llm-config.png}
              \caption{LLM configurations}
              \label{fig:llm}
          \end{figure}



    \item Context Preparation and Session Retrieval

          To effectively detect anomaly we can not just examine a single log entry, it is
          depending on the surrounding logs in the same session. To create this context for
          LLM, I implement the system so that it can retrieve all log entries that
          sharing the same anomaly's BlockId from the processed CSV file. This process
          involves four steps which is shown in Figure \ref{fig:context}.
          First, the system queries the database to find the file path of
          CSV file in MinIO. Next, the CSV is downloaded from MinIO using chunk
          approach to prevent unnecessary memory usage. The parsed log entries are
          then filtered to keep the one that match the target BlockId. Finally,
          the filtered logs are sorted chronologically based on their LineId to ensure
          the original execution order.

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{context.png}
              \caption{LLM context preparation}
              \label{fig:context}
          \end{figure}


\end{enumerate}


\subsection{Prompt Engineering}

Prompt engineers plays an important role on how our Large Language Model understand
the context and response. In this system, prompts are designed to guide the model specific
for anomaly log reasoning. The system uses two-tier prompting strategy which includes
system prompt and user prompt.

\begin{enumerate}[label=\arabic*.]

    \item System Prompt: Role Definition

          The system prompt defines to role and behavior of the LLM before it process tasks.
          In this project, th prompt instructs the LLM to become a expert in HDFS log analysis
          and anomaly detection. In addition, the prompt strictly enforce the output
          format requirement for LLM to JSON format only. This approach is necessary because
          it will prevent other element like markdown text, emojis, or code blocks that usually
          be generated by LLMs and keep the response in consistent format for later parsing.


    \item User Prompt

          The user prompt constructs a guide for LLM to analyze the log sequences that are
          detected as anomaly. Using the context that is prepared, the prompt let the model
          explore the all the related event to give a more concise explanations and explanations
          actions.

\end{enumerate}



% - System prompts
% - User prompts with context
% - Few-shot examples (if any)


\section{Frontend Implementation}

This section will describes the implementation of the frontend layer of the system which provides the
interface for user to upload log, visualize anomaly and analysis results. The frontend is implemented
using React framework which offer the ability to build a responsive and scalable frontend for the system.


\subsection{React Application Structure}

The frontend of the system organizes the components by grouping them by functionality. This approach
improves the scalability and maintainability of the code base. The primary source directory is place
under \texttt{frontend/src/} and structured as follows.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{frontend-structure.png}
    \caption{React frontend structure}
    \label{fig:frontend}
\end{figure}

Figure \ref{fig:frontend} shows the frontend structure of the system which provides clear
separation between components to make it easy to develop and maintain. For instance,
authentication-related components such as Login.tsx, Register.tsx, AuthContext.tsx
are separated from analysis components like LogUpload.tsx or Results.tsx. In addition,
the frontend is developed using TypeScript XML (.tsx) in stead of JavaScript XML (.jsx)
which provides a better static type checking and  support with better with error detection.

% \begin{enumerate}[label=\alph*.]

%     \item 
% \end{enumerate}
% - Component organization
% - Routing setup

\subsection{Key Components Implementation}
\begin{enumerate}[label=\alph*.]

    \item Authentication Flow

          The authentication flow is responsible for verifying user identity, it also manages
          the session state and restrict access to routes that are protected. This will
          ensure only authenticated people can access to the system and perform log
          analysis tasks.

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.85\textwidth]{login-sequence.png}
              \caption{Sequence diagram for authentication flow}
              \label{fig:login-sequence}
          \end{figure}

          The Figure \ref{fig:login-sequence} describes the Authentication flow.
          The process begins when the user enters their credentials (username and password)
          into the login form of Login component and clicks "Login" button. The Login component
          call the login(username, password) function from AuthContext. The AuthContext then
          send POST request to the API endpoint /auth/token. The FastAPI backend verifies user
          credentials with information that stored in database.
          If the credentials are valid, the backend generates a JSON Web Token (JWT) and
          return it to the API service. The AuthContext receive the JWT, stores the token
          in client local storage. To retrieve the user’s details, AuthContext requests /auth/me with the Bearer token and
          the backend validates the JWT and returns user details. Finally,
          The client updates authentication state and redirects the user to the dashboard.


    \item Log Upload Component

          The log components provides the main interface for user to submit log files for
          anomaly detection. The upload interface is implemented as a React components
          that can handle file selection, upload progress tracking, and result viewing.
          The component can validate file types on the client side before send to backend
          to prevent unnecessary network traffic.

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{logupload-sequence.png}
              \caption{Sequence diagram for upload log}
              \label{fig:logupload}
          \end{figure}

          The Figure \ref{fig:logupload} describes the log upload phase.
          The process begins when the user selects a log file and clicks the
          upload button in the frontend Upload Component. The frontend
          send a POST request to the backend. The backend then valid the file
          then upload to MinIO storage. Once uploading success, the log processing
          task is created and publishing to Redis message queue. Finally, the backend
          returns a success response to the frontend with job ID allow user to
          track processing status.



\end{enumerate}



\chapter{RESULT ANALYSIS}

This chapter presents a comprehensive evaluation of the proposed log anomaly detection
system. The analysis focuses on the overall effectiveness of the machine learning models
for log anomaly detection, the overall performance of the system and the quality of
LLM generated explanations.

\section{Machine learning models evaluations}

To measures the performance of each model, I use  precision, recall and F-measure,
which are the most commonly used metrics, to evaluate the
accuracy of anomaly detection methods as we already have
the ground truth (anomaly or not) for the dataset. As shown below, precision
measures the proportion of how many anomalies are correctly identified in
those reported.

    {\large
        $$
            Precision = \frac{\# Anomaly detected}{\# Anomaly reported}
        $$
        $$
            Recall = \frac{\# Anomaly detected}{\# All anomalies}
        $$
        $$
            F-measure = \frac{2 \times Precision \times Recall}{Precision + Recall}
        $$}


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{models-score.png}
    \caption{Training accuracy}
    \label{fig:train-accuracy}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.02\textwidth]{test-score.png}
    \caption{Testing accuracy}
    \label{fig:test-accuracy}
\end{figure}


Based on the experiment results that capsulates in Figure \ref{fig:train-accuracy} and
Figure \ref{fig:test-accuracy},the Decision Tree
model demonstrates that most superior performance among all approaches.
It achieve near-perfect score for all three evaluated metrics on both training
and testing data. SVM algorithm also perform well with F1 score approach 0.964
but the algorithm itself is more complex and harder to understand. Unsupervised methods
such as PCA  high precision score but low in recall, there for
decrease its ability to detect diverse pattern. Forest and Logistic Regression
provide moderate score but much lower than Decision Tree model.  As the result,
Decision Tree model achieves the optimal accuracy and operational simplicity
for my production log anomaly detection framework.


\section{LLM responses}

The LLM is provided with session-level log sequences instead of just a single log entry.
This context helps improving the quality of the explanations and allows models to identify
root cause more effectively. The LLM responses have a clear consistent structure which
contains explanations why a log sequence is labeled anomaly, root causes analysis,
severity level and recommended actions. In most cases, the recommendations are sound reliable
and detail enough. Figure \ref{fig:llm-res} show an example of generated analysis from LLM.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{llm-res.png}
    \caption{Example of a LLM response}
    \label{fig:llm-res}
\end{figure}



\section{User Interface Effectiveness}

The frontend provides a clear and linear workflow: log upload, job monitoring, result
visualization, and anomaly explanation review.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{use-case.png}
    \caption{Use Case Diagram - AI Log Analysis Web Application}
    \label{fig:usecase-diagram}
\end{figure}

Figure \ref{fig:usecase-diagram} provides use cases that exist in this project.
There are two main actors that defined in the system. Users is the primary actor
who interact with the system to analyze log data and administrators who inherits all the
capabilities of the standard User but also have ability to view detail of all Users
and the system insights. Fundamental features that available to a user are register,
login, upload log file , view job status and analyzed result.


Some main pages of the frontend which includes both User and Admin dashboard
will be present as follow.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{login-page.png}
    \caption{Login page}
    \label{fig:login-page}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{dashboard-page.png}
    \caption{User dashboard page}
    \label{fig:dashboard-page}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{upload-page.png}
    \caption{Log uploading page}
    \label{fig:upload-page}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{llm-res.png}
    \caption{LLM responses page}
    \label{fig:llm-response}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{user-manage.png}
    \caption{User management admin page}
    \label{fig:users}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{file-manage.png}
    \caption{File management admin page}
    \label{fig:file}
\end{figure}



\section{Limitations}

Despite the system's promising results, there are still several  limitations were identified
during development and evaluation.

\begin{enumerate}[label=\arabic*.]


    \item Dataset and Labeling Constraints

          The system is currently relying on HDFS labeled dataset for model training.
          which can not accurately detect anomalies in real-world log data.
          If apply the models to non-HDFS logs, the result is would likely to undergo
          a significant degrade in accuracy. Since the supervised models can only detect
          patterns present in the training datam unseen or newly failures and anomalies
          may remain undetected unless new labeled sameples are adding the dataset for
          the retrain.

    \item Limited Explainability

          Another important limitations is the limited explainability of anomaly detection
          decisions produced by the machine learning models. The Decision Tree classifier
          determines anomaly based on the decision rules that take from event count vectors.
          However, these internal decision paths such as which features or rules that triggered
          the anomalies are not exposed to the users. As a result, the system lack of insights
          into each decision, making it difficult to actually validate to model behavior.
          While the integration of Large Language Models may mitgate the issue by providing
          the human-readable explainations but yet not directly grounded in the root decision
          logic of the Decision Tree.

    \item Manualy Interaction

          The proposed system is relying on manual log upload interaction which is not practical
          in  real-world deployment. This design limits the ability to immediately detect and
          response in reality where logs are generated continuously and failures must be identified as
          early as possible. Therefore, in practical production settings, a automated log collection
          solutions such as log agents or streaming pipeline are highly required to continuous
          ingestion and near real-time detection.
\end{enumerate}





\chapter{CONCLUSION AND RECOMMENDATION}

\section{Key findings}

First, the Drain parsing technique proved the effectiveness in extracting HDFS raw logs into
structured data despite variations in dynamic parameters such as IP addresses and
block identifiers. Second, the Decision Tree classifier showed the strong performance
in detecting anomalies with near-perfect score. Third, the integration of  large language models
for reasoning successfully transformed classification to clear and specific explanations. However,
the supervised learing approach in this study required labeled training data which may not
practical when unseen or new failures arrive.

\section{Recommendations for future work}

There are several direction for future research that are highly recommended.
First, extended the framework to support multiple log format instead of just for HDFS log
in the current implementation. This would significantly improve the system flexibility.
Secondly, in the future a other machine learning approaches will be investigated to
address the requirements for static, pre-labeled dataset of Decision Tree algorithms.
This limitations of the current implementation could degrade the system detecting
performance when deploying in real-world environments.
Finally, the current implementation will be enhanced to have to ability for
real-time streaming and perform more automated actions rather than relying on manual
operations.

\section{Conclusion of the research}

This thesis presented a comprehensive framework for anomaly detection in HDFS logs.
The research address some fundamental challenges in automated log analysis by developing
a system that can combines traditional approaches with modern interpretability solution.
The proposed solution implemented a end-to-end pipeline including transform raw logs
into structured data using Drain algorithm, then Decision Tree classifiers is utilized
to detect anomalies in log sequences and provides it to large language models for
explanations and mitigation recommendations generation. This implementation can bridge
the gap between anomaly detection and operational decision making.
While there are still some
limitations in the research  such as the lack of
real-time streaming capabilities, reliance on
labeled training data, the study provides a practical and valuable foundation
that can address challenges in automated log analysis and suggest
solutions for continued development.


\bibliographystyle{plain}
\bibliography{references}

\newpage

\begin{center}
    \textbf{\large{Plagiarism and AI checked}}\\[20pt]
\end{center}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{check2.JPG}
    % \caption{Login page}
    % \label{fig:login-page}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{check.png.JPG}
    % \caption{User dashboard page}
    % \label{fig:dashboard-page}
\end{figure}


\end{document}