\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{graphicx, float}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage[skip=10pt plus1pt, indent=40pt]{parskip}
\usepackage{tocloft}
\usepackage{cite}
\usepackage{titlesec}
\usepackage{times}
\usepackage{natbib}
\usepackage{url}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{bookmark}
\usepackage{enumitem}
\usepackage{pgfgantt}
\usepackage{booktabs}
\usepackage{array, multirow}



\renewcommand{\cftchapfont}{\bfseries}
\renewcommand{\cftchappagefont}{\bfseries}
\renewcommand{\cftchappresnum}{Chapter }
\renewcommand{\cftchapaftersnum}{:}
\renewcommand{\cftchapnumwidth}{6em}

\titleformat
{\chapter} % command to format
[block] % shape: hang, display, block, frame 
{\bfseries\Large \centering}  % format of label + chapter title
{CHAPTER\ \thechapter:} % label "Chapter 1:"
{1.5ex} % separation label - chapter title
{} % code before

\renewcommand{\baselinestretch}{1.5} 

\graphicspath{{img/}}

\begin{document}
\pagenumbering{roman}

\begin{center}
    \textbf{VIETNAM NATIONAL UNIVERSITY OF HO CHI MINH CITY}\\[6pt]
    \textbf{INTERNATIONAL UNIVERSITY}\\[6pt]
    \textbf{SCHOOL OF COMPUTER SCIENCE AND ENGINEERING}\\[50pt]
\end{center}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{iu-logo.png}\\[50pt]
\end{figure}

\begin{center}
    \textbf{\huge {Anomaly Detection in HDFS Logs}}
    \textbf{\huge {using Machine Learning Integrated with
            LLM-Based Mitigation}}\\[50pt]

    \textbf{By}\\
    \textbf{Nguyen Hoang Quan}\\[20pt]
    \textbf{The thesis submitted to School of Computer Science and Engineering in partial fulfillment of the requirements of the degree of Bachelor of Engineering of Information Technology}\\[80pt]

    \textbf{Ho Chi Minh, Viet Nam}\\
    \textbf{2025}
\end{center}

\newpage
\newgeometry{
    % Define the margin values:
    left=1.5in,
    right=1in,
    top=1.5in,
    bottom=1.5in,
    % Optional: Define the paper size if you haven't already
    % a4paper 
}


\begin{center}
    \textbf{\Large{Anomaly Detection in HDFS Logs using Machine         Learning Integrated
            with LLM-Based Mitigation
        }}\\[80pt]

\end{center}

\begin{flushright}
    \textnormal{APPROVED BY: \rule{6cm}{0.5pt}}
\end{flushright}


\newpage

\begin{center}
    \textbf{\large{ACKNOWLEDGEMENTS}}\\[20pt]
\end{center}

First and foremost, I would like to express my sincere gratitude to the School of Computer Science and Engineering for providing a supportive academic environment and the essential resources that made this thesis possible. \\[10pt]
I am deeply thankful to my advisor, Dr. Le Hai Duong, for his invaluable guidance, encouragement, and constructive feedback throughout the development of this thesis. His expertise and commitment has greatly enriched both my technical understanding and research direction. \\[10pt]
I would also like to extend my appreciation to all lecturers and staff members who have contributed to my academic journey with their instruction, support, and mentorship.
Special thanks go to my family and friends for their constant support, motivation, and understanding during the preparation of this work.
Lastly, I would like to acknowledge the developers and the researchers behind the resources for providing the foundation upon which this project was built. \\[10pt]
This thesis would not have been possible without the guidance, resources, and support from all of the above.


\newpage
\tableofcontents

\newpage
\listoftables

\newpage
\listoffigures

\newpage

\begin{center}
    \textbf{\large{ABSTRACT}}\\[20pt]
\end{center}

Anomaly detection is essential for managing today’s large-scale distributed systems, where system logs are a key resource for identifying unusual behavior. Traditionally, system operators relied on manual inspection methods such as keyword searches and rule-based matching. However, due to the massive volume and complexity of modern system logs, manual approaches are no longer practical. To tackle this, many automated log-based anomaly detection methods have been proposed. Still, developers often struggle to choose a suitable method, as there hasn't been a clear comparison of these approaches.

In this research, I will propose a solution to addresses the fundamental challenge of automated log analysis. Specially, the research tackles three interconnected problems: (1) automated parsing of diverse log formats into structured templates, (2) anomaly detection in high-volume log streams, and (3) generation of contextual, actionable recommendations for identified issues.

Previous research has established some foundational approaches including the algorithms for log parsing and machine learning techniques for anomaly detection. However, existing solutions typically focus on individual components rather than providing end-to-end integration. Most academic implementations lack production-ready deployment architectures, user-friendly interfaces, and the integration of modern Large Language Models (LLMs) for intelligent recommendations—creating a significant gap between theoretical algorithms and practical deployment. Therefore, this research is important since it close the gap between academic log analysis algorithms and production-ready systems.

Furthermore, the research establishes a framework for integrating emerging LLM capabilities into traditional system administration workflows, suggesting broader implications for AI-assisted DevOps practices. The findings indicate that intelligent automation of log analysis is not only technically feasible but can significantly enhance organizational capabilities in system reliability, security monitoring, and operational efficiency.

\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

\chapter{INTRODUCTION}
\section{Background of the study}


In modern software systems, log files serve as critical sources of information
for system monitoring, debugging, troubleshooting, and security analysis. As applications or systems scale and increase its complexity,
the volume of generated log data gets bigger exponentially. Therefore, traditional manual approaches
for log analysis like manually examine through log files using basic tools such as grep or text
editors, has become less efficient and more defective \cite{author2023}. As the result,
it is becoming more difficult to detect anomalies within large scale system.

Over the year, a lot of automated log-based methods have been introduced to help detecting
system anomalies. These approaches usually require raw log preprocessing techniques, feature extraction
and machine-learning-based algorithms for processing vast amounts of
unstructured log data efficiently, identifying potential issues before they
escalate into critical failures. Recent advancements in natural language processing
and large language models (LLMs) have also increased the potential for
intelligent log analysis systems. However, despite these development, traditional machine-learning techniques and LLM-based approaches have yet to be efficiently integrated
into a single, cohesive log analysis framework.

This study focuses on leveraging existing machine learning approaches where log parsing and anomaly
detected are used, and augmenting them with large language model to provide  actionable insights and helpful recommendations.


\section{Problem Statement}

Despite the important role of log analysis that play in making the system more secure and reliable,
several significant challenges still persist in current practices:

\begin{itemize}
    \item \textit{Limited interpretability.} In log anomaly detection,
          the ability to interpret model's outputs is important for people who
          work as system administrators or analysts to effectively action to
          the alerts. They need to understand which log entries may be responsible
          for the detected abnormality. Yet, many traditional approaches only
          provide basic classified prediction without any explanation. As a result,
          engineers still have to perform further manual root cause analysis
          which in large-scale and complex systems becomes an very time-consuming
          and heavy task.

    \item \textit{Poor adaptability.} Many existing methods rely on a predefined
          set of log event templates during feature extraction phase (This phase will use the set of log event templates generated by
          log parsing to create numerical features for machine learning models \cite{SARHAN2024205}).
          However, as applications scale up and expand  in term of feature,
          new and unseen log events will definitely appear. Therefore, adapting to these
          changes require retraining models from scratch which make the systems become less practical
          in dynamic environments.

    \item \textit{Poor adaptability.}

\end{itemize}

\section{Objectives of the Study}

This study will perform the investigation on existing anomaly-detection
methodologies, compares the performance of different machine-learning
models, and develops a practical framework that integrates large language
models (LLMs) to automate and improve anomaly detection in real-world scenario.



\section{Limitations of the Study}


\chapter{LITERATURE REVIEW}

\section{Introduction to system log}

System logs, also known as event logs or audit trails, are automatically generated by
a computer system (its operating system, services, applications) that record events, changes and error that
occur inside that system. These log files serve an important role in monitoring system
health, diagnose failures, detecting anomalies, and ensure system security and compliance \cite{STUDIAWAN20191}.
System logs usually follow a semi-structured format which consist of several fields that capture
both the context and content of the event.\\[10pt]


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{hdfs-log.png}
    \caption{This is the descriptive text that explains the figure.}
    \label{fig:hdfs-log} % The label for cross-referencing
\end{figure}

Figure 2.1 show a log snippet that was generated by HDFS DataNode, one of the core storage components in the Hadoop ecosystem
(a more detail explanation of this system will be provided below).
These log entries are created from the storage operations of the system such as
data writes, packet handling, and communication between DataNodes. A log entry typically follow a structure as follow:

\begin{itemize}
    \item \textit{Timestamp: 2016-10-02 12:24:52,337.} Indicates the exact moment when the system processed the event.

    \item \textit{Log level: INFO.} Shows that the entry reports normal operational activity

    \item \textit{Log message: org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace:...} Log messages provide a structured description of system behavior,
          capturing events, performance indicators. This is the most important part of a log entries and therefor become the
          foundation for monitoring, auditing and anomaly detection.

\end{itemize}

Given their important role in providing such essential information about the system, logs must be reliably collected,
centrally stored and appropriately maintained  to enable accurate monitoring, troubleshooting, and subsequent analytical processes.

\section{Traditional log analysis method}

Traditional log analysis have relied on two primary approaches (cite something): manual log inspection and rule-based pattern matching systems.
While these techniques are the foundation of the system administrators fields and troubleshooting, it were used during the time
when the system complexity and log volumes were much smaller than nowadays modern systems. Understanding the strengths and limitations of
these approaches will provide you a better context and insights of logs which will enhance your log analysis systems.


\subsection{Manual Log Inspection}

Manual log inspection is one of the earliest and most straightforward methods used by system
administrators to identify issues. In this approach, developers or administrators will
directly operate searching on raw log files using command-line tools such as \verb|grep| , \verb|tail|, \verb|less|,
to locate error logs or correlate timestamp of the incidents. While feasible for small log volumes, this method is very
time-consuming, highly error-prone and relies heavily on an operator’s familiarity with the system. Therefore, this approach
is no longer a feasible solution for detecting anomalies in large-scale systems which may produce millions of log entries
per hour. For instance, in 2013 the Alibaba Cloud system was reported to generate approximately 100 to
200 million log lines in one hour \citep{6410318}.

\subsection{Rule-Based Systems (Regex and Pattern Matching)}

To counter the limitations of manual log review, developers created rule-based system that used fixed rules, regular expressions (regex),
and predefined patterns to filter, parse, and identify specific events when a specification conditions are met.
For example, system operators can create rules to detect keywords such as "ERROR", "FAILED" or extract key information
from log entries. Rule-based methods offer some advantages: they are easy to implement and effective for detecting
obvious or recurring errors. Many traditional monitoring tools such as Nagios, Splunk (early versions)
\cite{splunk_field_extraction}, and Logstash \cite{logstash_grok},
they are heavily depend on popular pattern-matching techniques to filter, categorize, and index log data.

However, the effectiveness of rule-based approaches highly depend on the completeness and accuracy of the
hand-crafted predefined rules. It will become more defective in a constantly evolving environments where logs
format change frequently and new types of anomalies will appear which do not match the existing patterns. Maintaining
large sets of regex rules is also a labor-intensive action and require continuous manual updates on patterns which is
unsuitable for adaptive anomaly detection.



\section{Log parsing}


\section{Machine learning and deep learning for log anomaly detection}

As system continuously scaled and logs volumes grew bigger, manual or rule-based approaches is
no longer available. As a result, researchers began to adopt machine learning to automate
anomaly detection. These methods are created to enable it to learn the structures, numerical representations,
and patterns of logs without depending on predefined rules or templates. These approaches are often categorized as
supervised or unsupervised learning.

\subsection{Supervised learning}

\begin{enumerate}[label=\alph*.] % Sets the label format to (a), (b), (c)...
    \item Support Vector Machine (SVM)

          Support Vector Machine is one of the fundamental methods that proposed
          by Vapnik \cite{vapnik1998statistical} which is used for binary classification.
          The basic idea of this method is as follow. For nonlinear separable data, we
          can transform it to higher-dimensional space where the data becomes linearly separable.
          This allows the classifier to construct an optimal separating hyperplane between normal
          and anomalous log patterns. In log detection context, anomaly logs will be treated as negatives
          and normal one will be served as positive examples. But using this method for detecting
          anomalies often often suffers from class imbalance, where the positive examples is outnumber
          the negatives. As the result, the classifier will overfit the anomalous logs, which make
          it become weak in detecting new occur, unseen logs.

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.7\textwidth]{svm.jpg}
              \caption{SVM classification}
              % \label{fig:my_image} % The label for cross-referencing
          \end{figure}

    \item Decision Tree (DS)

          Decision tree is one of the earliest machine learning approaches that apply to
          anomaly log detection and are defined by Quinlan \cite{quinlan2014c4} as "powerful
          and common tools for classification and prediction". This method recursively
          splitting data based on its features then create a tree-like structure where
          each leaf node represents a prediction class (normal or abnormal).
          When apply to log anomaly detection, we have to transform log entries into
          structured numerical features before feed it into decision tree algorithm.
          The applications of decision trees to log-based anomaly detection has been
          proposed in many research with promising results. He et al. \cite{7774521}
          did some experiments with anomaly logs in HDFS dataset using decision tree classifier and
          and prove the effectiveness of it. The approach not only achieve high accuracy
          but also provide interpretable rules that allows others to validate and refine
          them as they wish.
          In context of this research, decision tree served as a foundation machine
          learning algorithm for detecting anomalies in log data. The specific implementations
          and results of using this algorithm will be discussed more upon in subsequent sections.

          % \begin{enumerate}[label=\arabic*.]
          %     \item The first item in the list.
          %     \item The second item in the list.
          %     \item The third item in the list.
          % \end{enumerate}



    \item The final item summarizes the topic.
\end{enumerate}

\subsection{Unsupervised learning}

In many situation when anomaly data is not labeled or unavailable, the supervised
learning is thereby become impractical. As a result, unsupervised learing methods
have been adopted widely for log anomaly detection. These approaches aim to analyze
data without requiring labeled examples by discovering the characteristics of the
data itseft. This section will examine some popular unsupervised approaches that are
currently applied to log analysis.

\begin{enumerate}[label=\alph*.] % Sets the label format to (a), (b), (c)...

    \item Isolation Forrest

          Isolation Forrest is a machine learning algorithm that was introduced by Liu et al
          \cite{liu2012isolation} in 2012. His core principle of isolation forrest is that
          anomolies are more accessible to isolate than abnomalous points \cite{Longberg2024}.
          The algorithms will construct forrest of random binary trees to separate each
          data point. Each tree us built by randomly selecting a feature from the dataset
          and a split value within the feature's range, then repetitive split the
          data until it reach the maximum depth or the instance is isolated.

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{isolation-forrest.png}
              \caption{Isolation Forest}
              % \label{fig:my_image} % The label for cross-referencing
          \end{figure}

          Figure 2.3 demonstrates the isolation capabilities of Isolation Forest algorithm.
          In the left, the data point is seperated with only one split, indicating a high anomaly score.
          On the other hand, data point on the right requires more splits,  suggesting it’s a
          nominal data point
    \item Principal Component Analysis (PCA)

          Principal Component Analysis is a common technique that widely used for dimensionality reduction
          \footnote{Dimensional reduction is the process of reducing the number of input variables
              (features) in a dataset keeping as much essential information as possible.}
          in a dataset. By doing so, PCA algorithm helps simplifying complex dataset, reducing
          the redundancy among features, and highlighting the most important patterns in the data
          \cite{jolliffe1990principal}. In anomaly detection, it first converts each log sequence
          into event count vector by counting how many time each type of log event appear in the
          sequence. Next, the PCA algorithm \cite{lee2006application,abdi2010principal}
          will capture the main patterns of normal log behavior then project those vectors to
          a learned space (also known as normal space). However, when a log sequence contains
          anomaly logs such as missing events or unusual log messages, PCA algorithm will mark
          it as abnormal behavior by calculating the Square Prediction Error (SPE) \cite{jackson1979control}
          , which measures the deviation from the learned normal patterns.

    \item K-Means Clustering



\end{enumerate}

- ml techniques: https://arxiv.org/pdf/2307.16714

\section{Large Language Models (LLMs) in Log Analysis}

Parsing with LLMs instead of templates:

% https://zeyang919.github.io/paper/LLMParser.pdf?utm_source=chatgpt.com

Explainability + anomaly detection:

\chapter{METHODOLOGIES}


\section{Overview}

This chapter describes the methodology that used to design, implement, and evaluate the
proposed log anomaly detection framework. The methodology follows a pipeline that begins with
data collection and preprocessing, then extract the features and using machine learning techniques
to detect anomolies. Large language models (LLMS) are also integrated to address the limitations
of the traditional approaches, giving more insights and contextual understanding of detected
anomolies. This chapter is organized to present each stage of a framework, from system arichiteture
and how data is prepared to model Implementation and evaluation.

\subsection{Research Approach}
In this study, I adopt a design-based experimental research approach that combines traditional
machine learning methods with large language model techniques. This approach will focus on
constructing a practical, user-friendly and production-ready log anomaly detection framework
by integrating proven traditional methods with LLM-based reasoning to addressing both the technical
and operational challenges of large-scale log analysis.

The methodology combines well-established components such as the Drain log parsing algorithms to
constructs structured log templates from raw log messages, Decision Tree or Isolation Forrest
machine learning models are compared to find the most effective one and implemented it as
detection instruments.

For data collection, this study relies primarily on secondary collection methods which uses public
and available log datasets like HDFS system logs or BGL logs dataset. There will be no primary data involving human
participants are collected. The data collection process includes log ingestion, parsing, feature extraction,
anomaly labeling (already prepared by experts), and preparation for model training and testing.

This study use a mixed-method of quantitative and qualitative analysis approaches. Quantitative analysis is utilized to evaluate the performance of detection models
using real statistics (e.g., detection rates and error rates) and comparison between models.
Furthermore, qualitative analysis is applied to the outputs that is generated by LLMs, focusing
on their ability to provide root cause explanations and mitigation suggestions.

Ethics are also considered in this study which relate to data usage and research integrity.
All datasets that I used are publicly available so it ensures no personal information is
exposed. Proper citation and acknowledgment of datasets, tools, and prior research are also maintained.
Additionally, this study is responsible for the LLM-generated outputs by put it as only suggestions
rather than automated direct action, and thereby preventing unintended operational impact.

\subsection{System Pipeline Overview}
The proposed framework follows a pipeline from transform raw systems logs into structured one and useful
recommendations through a sequence of stages. Figure \ref{fig:pipeline} illustrates the overall pipeline.

The pipeline begins with log preprocessing, where raw logs are parsed using the Drain algorithm. This
step will convert raw and unstructured log into sequence of structured events and extract event templates.

Structured log sequences will be the input for the processing step, where it will be transformed to
numerical representations and then fed to machine learning models for anomaly detection.

The final stage integrates large language models (LLMs) to enhance system interpretability. LLMs component
will perform semantic analysis based on detected anomalies and generate human-readable explanations about
root casues and useful suggestions. \\[10pt]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{pipeline.png}
    \caption{System pipeline}
    \label{fig:pipeline}
    % \label{fig:my_image} % The label for cross-referencing
\end{figure}


\section{System Architecture and Design}
This section presents the system architecture and design of the proposed log anomaly detection framework.
We will focus on the structural composition of the system and also show their responsibilities, and interactions.
The architecture is designed to make the process of large-scale log data more efficiently while enabling to
integrate with traditional anomaly detection techniques and large language models (LLMs).

\subsection{Architectural Overview and Design Principles}

The proposed log analysis system follows a simple architecture but highlight the
practicality and ease of integration to support real-world problems. The systems is desgined around some
key principles:


\begin{enumerate}[label=\arabic*.]
    \item Asynchronous Processing

          The architecture apply asynchronous task processing to handle computationally heavy workload without
          affect user interface. Therefore the system can effective process large log files while also maintain
          good user interaction.

    \item Modular Service Design

          Each component in the system is designed as an independent module with clear responsibilities to maintain or test
          more easily, and also increase the ability of horizontally scaling of individual components.

    \item Data Layer Separation

          The system create a clear separation between metadata management and object storage that using
          MinIO for large-scale objects and PostgreSQL for logs structured metadata. This design optimizes
          the performance by addressing the data access pattern problem where PostgreSQL is suitable for
          structured queries and indexing whereas MinIO offer high throughput and cost efficiency for
          actual log storage.

    \item Multi-Stage Processing Pipeline

          In my log analysis system, I implemented a multi-stage pipeline which consists of parsing, anomaly detection,
          and intelligent analysis phases. This allows me to test and integrate different algorithms and models
          that is the most suitable for the system without disrupting the entire workflow.

\end{enumerate}

\subsection{High-Level System Architecture}

The system uses a layered, distributed architecture which is designed to support efficient log ingestion and
anomaly detection. Each layer is responsible for a distinct functionality ensuring the ease for development
and maintenance.

\begin{enumerate}[label=\arabic*.]

    \item Presentation layer

          This layer provides a user-friendly interface between end users and the log analysis system. It is a
          single-page application (SPA) that built on React framework, allowing users to upload
          log files, view processing status and final analyzed results. The system also develop
          authentication through JWT-based mechanism and role-based accesses for better security.

    \item API Gateway Layer

          The API gateway layer is responsible for create a communication between the frontend and backend.
          This layer expose REST API endpoints which will handle request routing, input validation and give back the
          responses by using FastAPI technologies.

    \item Service Layer

          The service layer hold the core business logic of the system. These services
          include authentication, handling log files, and managing model detection logic.
          and LLMs analysis.
          By separating functions into discreate services, the architecture allows each service to
          work independently and easy to change as you wish.

    \item Data Processing Layer

          Data processing is one of the most important steps in the entire workflow, so this layer
          is built very carefully. Raw log messages are parsed into structured one and extract event
          templates using Drain algorithm. Then using the structured logs to ingest to machine learning models
          and save as serialized file for real-time inference. Anomalies is then analyzed by LLMs
          to generate explanations about how it mark as anomaly and mitigation suggestions.

    \item Storage Layer

          The storage layer is designed to handle how log data will be stored and accessed effectively.
          MinIO is used to store actual raw log files that users upload to the server.
          I utilized this technology because its object storage allows storing and access large volume
          of data with high speed and low cost. Whereas PostgreSQL is chosen to manage structured metadata
          due to its ability to create complex structured queries. An in-memory cache (Redis) is also
          employed in this system to support fast access to data such as task queues.


    \item Task Management Layer

          To support asynchronous processing as I mentioned in section 3.2.1, the system also create a
          layer for task management. Each step such as parsing, anomaly detection, and LLM inference will
          be define as tasks and put to message queues. Celery workers will take tasks from queue and execute
          in the background, thereby parallel execution is made possible.

\end{enumerate}



\section{Dataset preparation}

The HDFS log data set is the most frequently used data set for anomaly detection methods and
thus also the main focus point of this study. The logs were collected from the Hadoop Distributed
File System (HDFS), which runs on the Amazon EC2 platform. This system is designed to run on
commodity hardware and allows users to store and process large files. Each log event have one or more
block identifiers which will enable grouping logs into sequence of events. And in fact, the sample
logs snippet show in Figure \ref{fig:hdfs-log} are taken from HDFS data set. The core idea behind
the anomaly detection in this dataset is that some data blocks is fail to be processed by the
system. When such failures happen, the blocks generate log events that differ from normal processing behavior.
Therefore, the entire sequence should be detected as anomalous \cite{xu2009detecting}.

The data was originally collected by Xu et al. from a production hadoop cluster comprising
more than 200 nodes. The dataset was labeled by domain experts to distinguish normal
and anomalous execution flows. In this study the HDFS dataset version is taken from project
Loghub \cite{zhu2023loghub}. The total lines of log messages in this dataset version is
11,175,629 but a close inspection of the dataset shows that approximately 22,000 lines are missing in comparison to the original data set for
unknown reasons. However, the remaining data still provides a sufficient sample for
anomaly detection analysis and techniques evaluation.


\section{Log Parsing Methodology}




\section{Feature Extraction / Representation}
\section{Anomaly Detection Models}
\section{LLM Integration Method}





\chapter{IMPLEMETATIONS}
\section{Implementation Overview}

This chapter will present the implementation of the proposed log anomaly detection application. Each
components that are described in chapter 3 will be transformed to real functional code and integrate
in to the system. The scope of this chapter is focus on implementaion details including
configuration, library used, API designs and technologies that are used to build
end-to-end system.


\section{Implementation timeline}

The system implementaion is followed a structured timeline that consists of several
phases and each phase will focus on a specific tasks and will be described as follow:



\begin{itemize}
    \item \textit{Phase 1 (Week 1)}

          This initial phase focused on prepare and research background knownledge for the project. Some
          major activities are:

          \begin{itemize}

              \item Creating a literature review about log parsing, anomaly detection techniques,
                    and LLMs for log analysis.

              \item Understanding about HDFS datatset, how it was generated, and its structure.

              \item Defining sytem requirements and architecture.

          \end{itemize}

    \item \textit{Phase 2 (Week 2): Data Preparation}

          In this phase, raw actual log will be collected, and prepared for use in subsequent steps.

          \begin{itemize}

              \item Downloading raw HDFS dataset from LogHub repository and validating data.

              \item Using a small sample in the data set to understand the event structure.

              \item Set up a directories just for data storage purposes.


          \end{itemize}

    \item \textit{Phase 3 (Week 3-5): Backend Infrastructure Setup}

          The fundamental backend environment was set up carefully for later implementation:

          \begin{itemize}

              \item Setting up the FastAPI project structure, base routers, and middleware.

              \item Configuring PostgreSQL database, Redis, and MinIO containers using Docker.

              \item Designing database schema.

          \end{itemize}


    \item \textit{Phase 4 (Week 6-7): Log Parsing module development}

          This phase mainly focused on implementing a wrapper for Drain parser in order to integrate into
          the system, which includes some major steps:

          \begin{itemize}

              \item Developing a Python script to wrap the original Drain algorithm.

              \item Creating Celery tasks for asynchronous parsing.

              \item Designing storage solution for parsed logs (MinIO + PostgreSQL)

          \end{itemize}


    \item \textit{Phase 5 (Week 8-9): Log Parsing module development}

          Some popular ML anomaly detection models were utilized and evaluated in this phase:

          \begin{itemize}

              \item Extracting event count vectors and features.

              \item Training and validating models using parsed event sequences.

              \item Storing trained models for reusing in the processing pipeline.

          \end{itemize}


    \item \textit{Phase 6 (Week 10-11): LLM Integration}

          This phase focused on implementing LLM for more understanding of anomalies:

          \begin{itemize}

              \item Developing the LLM service module for interacting with the model API.

              \item Structuring a prompt templates for explanation, and mitigation suggestions.

              \item Integrating into the pipeline as the final stage.

          \end{itemize}

    \item \textit{Phase 7 (Week 12-13): Frontend Development}

          A functional web interface is contructed and delivered in this phase using React technology:

          \begin{itemize}

              \item Developing the LLM service module for interacting with the model API.

              \item Structuring a prompt templates for explanation, and mitigation suggestions.

              \item Integrating into the pipeline as the final stage.

          \end{itemize}

    \item \textit{Phase 8 (Week 14): Testing, Optimization, and Debugging}

          This phase performs several tests and optimization to ensure product stability:

          \begin{itemize}

              \item Optimizing database queries and model loading.

              \item Refining UI responsiveness and user experience flow.

              \item Running test with a full datasets to ensure all stages work smoothly.

          \end{itemize}


    \item \textit{Phase 9 (Week 15-16): Documentation and Thesis writing}

          The final phase will consolidate all the work and prepare the system for thesis writing:

          \begin{itemize}

              \item Documenting code structure, APIs, preprocessing steps, and models.

              \item Preparing visualizations, diagrams, and comparison tables.

              \item Create overall structure for thesis and complete the final work.
          \end{itemize}

\end{itemize}



\section{Backend Implementation}

The backend of the log analysis system was developed using FastAPI due to its ease of use and high performance.
The implementation follows the layered architechture that introduced in Chapter 3 and therefore separate the
entire system into modular routers and components in order to maintain and scale more easily.

\subsection{FastAPI Application Architecture}

\begin{enumerate}[label=\alph*.]

    \item Application Initialization

          The main application entry point is defined in `src/main.py`, where all routers and middlewares are registered.
          The main application is also responsible for initializing CORS middleware and create all database
          tables that defined in SQLAlchemy models whenever the application start. This setup system features like Auth, Jobs
          Logs, to be managed by it own route files.


          \begin{figure}[H]
              \centering
              \includegraphics[width=0.6\textwidth]{mainpy.png}
              \caption{main.py file}
              \label{fig:main.py}

          \end{figure}


    \item Router Organization \\[7pt]

          \begin{table}[h]
              \centering
              % Sử dụng '|' để tạo đường kẻ dọc và '>{\ttfamily}l' cho phông chữ code
              \begin{tabular}{|l|l|}
                  \hline
                  \textbf{Router} & \textbf{Responsibilities}                      \\
                  \hline
                  auth\_router    & Login, registration, token generation          \\
                  \hline
                  log\_router     & File upload, metadata retrieval                \\
                  \hline
                  job\_router     & Job creation, status queries, result retrieval \\
                  \hline
                  ml\_router      & Model inference endpoints                      \\
                  \hline
                  llm\_router     & Explanation and mitigation generation          \\
                  \hline
              \end{tabular}
              \caption{Router Responsibilities}
              \label{tab:module_responsibilities_final}
          \end{table}

          Each domain in the system is seperated to different router to maintain the ability to
          scale, be readable when number of features and endpoint grows. The table \ref{tab:module_responsibilities_final}
          gives information about the total router that are implemented in the systems and their
          responsibilities. Routers itself will only communicate with service layers to mainin a well
          and clear application architecture.

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{router.png}
              \caption{Example of a router configuration pattern}
              \label{fig:router.py}
          \end{figure}


          Figure \ref{fig:router.py} captures a code snippet that represent a pattern used for
          implementing router configuration. Each router will contain several endpoints that
          directly relatect to that functional service layer. For example, the authentication
          router includes enpoints for user creation, get login users or send access tokens.
          This approach allows developers to organize the application better and avoid as much as
          confusion in source code.




\end{enumerate}
% - Code snippets from src/main.py
% - Router organization
% - Dependency injection
% - Database session managementPhase 


\subsection{Database Layer Implementation}
fSupport Vector Machine (SVM)
The database layer was built based on PostgreSQL database with SQLAlchemy ORM which allows developers
to interact with relational databases using Python classes and objects instead of using raw SQL queries.
All entities in the system such as users, logs, jobs and analysis are represented as ORM models ann then
mapped to relational database tables.

\begin{enumerate}[label=\alph*.]

    \item Database schema

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{schema.png}
              \caption{System database schema}
              \label{fig:database_schema}
          \end{figure}

          Figure \ref{fig:database_schema} depicts 4 core tables in the anomaly detection system, including \texttt{users},
          \texttt{log\_files}, \texttt{processing\_jobs} and \texttt{analysis\_results}. Each table serves a distinct pupose
          and are described as follow:


          \begin{itemize}

              \item Table users: This table will manage users information such as user name, password,
                    email adress and their role in the system.

              \item Table log\_files: This table stores metadata of a log file that uploaded by users,
                    and there will be a link to its actual storage location in MinIO.

              \item Table processing\_jobs: This table is primarily used for storing jobs information that
                    create by the system to track the Celery background tasks.

              \item Table analysis\_results: ML anomaly detection results for eacg analyzed log filed are
                    stored in this table with some information such as number of sequences, anomaly and normal sequences
                    count


          \end{itemize}

          The database schema also implement logical relationships between entities. Each user can upload
          multiple log files and create multiple processing jobs. The log file entity is a central reference
          point to connect both processing jobs and analysis results. This implement means each log file
          can be associated with multiple processing jobs, which happen when reprocessing is required due to
          any failures in the pipeline. In addition, each log file can also link to multiple analysis results
          to support the LLM integration stage.


    \item Model implementation
          \begin{figure}[H]
              \centering
              \includegraphics[width=0.86\textwidth]{data-model.png}
              \caption{System database schema}
              \label{fig:data-model}
          \end{figure}

          In this project SQLAlchemy ORM (Object Relational Mapping) is used to create
          a clean abstraction over a complex relational database. There are will be four
          model configuration file for four entities coordinatly. Each file is defined how
          information of a enity is store and connected within our database. Figure \ref{fig:data-model}
          shows how a model (Users model) is initialize in the project. The Users entity stores
          authentication information with server key attributes such as id which is identified
          for each user, username and password to log into the system. This file also so handle
          relationship with other entities, in this case, Users model have a one-to-many relationship
          with the LogFile entity. Each user can upload multiple log files, and each log file
          link to its owner via a foreign key.
\end{enumerate}

% fasdfasd
% - model diagram
% - detail

\subsection{API Endpoints Implementation}

Figure \ref{fig:endpoints} described all the endpoints that are exposed by the backend of
the system that cover authentication, log management, job processing, and LLM results generation.
Each endpoint is responsible for a specific functions in the entire processing pipeline.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.02\textwidth]{api1.drawio.png}
    \caption{API endpoints}
    \label{fig:endpoints}
\end{figure}
\vspace{20pt}

\textbf{\large{Endpoints implementation detail:}}

This section will provide detail of technical implementation of some important API endpoints.

\begin{enumerate}[label=\arabic*.]

    \item POST \verb|/api/logs/upload|

          This is the core enpoint that handles the file upload process in the system workflow.
          It exposes a interface for users to upload their log file, store them in object
          storage system, register a record in the datase and finally trigger an
          asynchronous prossing jobs for further analysis.

          The endpoint is implemented as HTTP POST route under path /api/logs/upload.
          It is defined using FastAPI's router and use asynchronous mechanism to
          handle request in order to ensure non-blocking I/O operations. When the backend
          receive a request from this endpoint, it first initialize a \texttt{LogService}
          instance which is a service layer that hold the business logic. The uploaded
          file is then validated for allowed extension. If the file extension does not match
          the \texttt{.log} extension, the method will immediately return HTTPException with
          status code 400 which is indicated a bad request error. The endpoint is continue
          to operate \texttt{LogService.save\_log\_file()} method which is responsible for
          uploading valid log files to MinIO and save its metadata to the database.
          Finally, the method return a structured results of several information.

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.9\textwidth]{upload-endpoint.png}
              \caption{Logs upload endpoints}
              \label{fig:Upload-endpoints}
          \end{figure}

    \item GET \verb|/api/auth/token|

          This endpoint is responsible for handling JSON Web Tokens (JWTs) to verify
          user within the system. Once the backend receive request, \texttt{UserService}
          will perform user finding, password verification and generate token.
          When the authentication is successfully done, the service will create JWT
          which contains user's information and attach it to request header when
          accessing protected resources.
          \begin{figure}[H]
              \centering
              \includegraphics[width=0.8\textwidth]{token-api.png}
              \caption{Token endpoints}
              \label{fig:token-endpoints}
          \end{figure}

\end{enumerate}




\section{Log Processing Pipeline}

This section provides detail implementations of multi-stage log processing pipeline
within the system. This pipeline is responsiple for transform raw logs into
structured data and perform anomaly detection with machine learning algorithm.

\subsection{File Upload and Storage}

\begin{enumerate}[label=\alph*.]

    \item MinIO Client Integration

          MinIO was selected as the object storage solution in this project to store raw log data
          due to its high performance and suitable for containerized deployments. The
          system implements a Python MinIOClient wrapper class that abstracts from
          the MinIO Python SDK.

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.8\textwidth]{minio.png}
              \caption{minio\_client.py file}
              \label{fig:minio\_client.py file }
          \end{figure}

          When initializing this class, the client create a connection using predefined
          access credentials and also vericate bucket with \texttt{\_ensure\_bucket\_exists()}
          method. This ensure the target bucket exist and automatically create it if
          necessary. The application also implement singleton pattern for MinIO client, where
          only one instance of \texttt{MinIOClient} is created and shared across the entire
          backend. This design allows system to maintain consistent connection with MinIO that helps
          uploading and storing operations faster.


    \item File Upload Implementation

          File upload mechanism is implemented inside \texttt{upload\_file()} method.
          The method first wrap log file data in \texttt{io.BytesIO} to transform data to
          file-like interface to that required by MinIO SDK. The the method perform
          an upload to existing MinIO bucket and a return a response with useful
          information when the upload is done.

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{upload-file.png}
              \caption{File upload method}
              \label{fig:file-upload }
          \end{figure}

    \item Unique Filename Generation

          To ensure files that uploaded to MinIO have the unique object name, the system develop
          a mechanism to create unique name for each object by combining timestamp, random
          characters and its original filename. As a result, each log file save to MinIO
          bucket will have result path as logs/{user\_id}/{timestamp}\_{uuid}\_{filename}.log.
          This strategy ensure the object name colision probability is extreme low and
          therefore reduce the error rate within the system. In addition, objects is store in
          separate user-scope directories which support better audition and efficient lookup.

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{minio-edit.png}
              \caption{Objects stored in MinIO}
              \label{fig:minio-name }
          \end{figure}
\end{enumerate}

% - MinIO client integration
% - File validation logic
% - Unique filename generation

\subsection{Asynchronous Processing with Celery}
- Celery configuration
- DatabaseTask base class
- Task retry logic
- Job status updates

\subsection{Drain Parser Integration}
- Parser service wrapper
- Temporary file handling
- Template extraction
- Output structuring

\subsection{Feature Extraction}
- Event counting
- Statistical features
- CSV conversion

\subsection{Decision Tree Anomaly Detection}
- Model training code
- Feature preparation
- Prediction pipeline
- Result storage

\section{LLM Integration}
\subsection{LLM Service Design}
- API client setup
- Context preparation

\subsection{LLM Service Design}
- System prompts
- User prompts with context
- Few-shot examples (if any)

% Recommendation Generation
\subsection{Recommendation Generation}
- Anomaly result processing
- LLM API calls
- Response Parsing

\section{Frontend Implementation}

\subsection{React Application Structure}
- Component organization
- Routing setup

\subsection{Key Components}



\chapter{CONCLUSION AND RECOMMENDATION}










\bibliographystyle{plain}
\bibliography{references}
\end{document}