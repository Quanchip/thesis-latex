\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{graphicx, float}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage[skip=10pt plus1pt, indent=40pt]{parskip}
\usepackage{tocloft}
\usepackage{cite}
\usepackage{titlesec}
\usepackage{times}
\usepackage{natbib}
\usepackage{url}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{bookmark}


\renewcommand{\cftchapfont}{\bfseries}
\renewcommand{\cftchappagefont}{\bfseries}
\renewcommand{\cftchappresnum}{Chapter }
\renewcommand{\cftchapaftersnum}{:}
\renewcommand{\cftchapnumwidth}{6em}

\titleformat
{\chapter} % command to format
[block] % shape: hang, display, block, frame 
{\bfseries\Large \centering}  % format of label + chapter title
{CHAPTER\ \thechapter:} % label "Chapter 1:"
{1.5ex} % separation label - chapter title
{} % code before

\renewcommand{\baselinestretch}{1.5} 

\graphicspath{{img/}}

\begin{document}
\pagenumbering{roman}

\begin{center}
    \textbf{VIETNAM NATIONAL UNIVERSITY OF HO CHI MINH CITY}\\[6pt]
    \textbf{INTERNATIONAL UNIVERSITY}\\[6pt]
    \textbf{SCHOOL OF COMPUTER SCIENCE AND ENGINEERING}\\[50pt]
\end{center}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{iu-logo.png}\\[50pt]
\end{figure}

\begin{center}
    \textbf{\huge {Anomaly Detection in HDFS Logs}}
    \textbf{\huge {using Machine Learning Integrated with
            LLM-Based Mitigation}}\\[50pt]

    \textbf{By}\\
    \textbf{Nguyen Hoang Quan}\\[20pt]
    \textbf{The thesis submitted to School of Computer Science and Engineering in partial fulfillment of the requirements of the degree of Bachelor of Engineering of Information Technology}\\[80pt]

    \textbf{Ho Chi Minh, Viet Nam}\\
    \textbf{2025}
\end{center}

\newpage
\newgeometry{
    % Define the margin values:
    left=1.5in,
    right=1in,
    top=1.5in,
    bottom=1.5in,
    % Optional: Define the paper size if you haven't already
    % a4paper 
}


\begin{center}
    \textbf{\Large{Anomaly Detection in HDFS Logs using Machine         Learning Integrated
            with LLM-Based Mitigation
        }}\\[80pt]

\end{center}

\begin{flushright}
    \textnormal{APPROVED BY: \rule{6cm}{0.5pt}}
\end{flushright}


\newpage

\begin{center}
    \textbf{\large{ACKNOWLEDGEMENTS}}\\[20pt]
\end{center}

First and foremost, I would like to express my sincere gratitude to the School of Computer Science and Engineering for providing a supportive academic environment and the essential resources that made this thesis possible. \\[10pt]
I am deeply thankful to my advisor, Dr. Le Hai Duong, for his invaluable guidance, encouragement, and constructive feedback throughout the development of this thesis. His expertise and commitment has greatly enriched both my technical understanding and research direction. \\[10pt]
I would also like to extend my appreciation to all lecturers and staff members who have contributed to my academic journey with their instruction, support, and mentorship.
Special thanks go to my family and friends for their constant support, motivation, and understanding during the preparation of this work.
Lastly, I would like to acknowledge the developers and the researchers behind the resources for providing the foundation upon which this project was built. \\[10pt]
This thesis would not have been possible without the guidance, resources, and support from all of the above.


\newpage
\tableofcontents

\newpage
\listoftables

\newpage
\listoffigures

\newpage

\begin{center}
    \textbf{\large{ABSTRACT}}\\[20pt]
\end{center}

Anomaly detection is essential for managing today’s large-scale distributed systems, where system logs are a key resource for identifying unusual behavior. Traditionally, system operators relied on manual inspection methods such as keyword searches and rule-based matching. However, due to the massive volume and complexity of modern system logs, manual approaches are no longer practical. To tackle this, many automated log-based anomaly detection methods have been proposed. Still, developers often struggle to choose a suitable method, as there hasn't been a clear comparison of these approaches.

In this research, I will propose a solution to addresses the fundamental challenge of automated log analysis. Specially, the research tackles three interconnected problems: (1) automated parsing of diverse log formats into structured templates, (2) anomaly detection in high-volume log streams, and (3) generation of contextual, actionable recommendations for identified issues.

Previous research has established some foundational approaches including the algorithms for log parsing and machine learning techniques for anomaly detection. However, existing solutions typically focus on individual components rather than providing end-to-end integration. Most academic implementations lack production-ready deployment architectures, user-friendly interfaces, and the integration of modern Large Language Models (LLMs) for intelligent recommendations—creating a significant gap between theoretical algorithms and practical deployment. Therefore, this research is important since it close the gap between academic log analysis algorithms and production-ready systems.

Furthermore, the research establishes a framework for integrating emerging LLM capabilities into traditional system administration workflows, suggesting broader implications for AI-assisted DevOps practices. The findings indicate that intelligent automation of log analysis is not only technically feasible but can significantly enhance organizational capabilities in system reliability, security monitoring, and operational efficiency.

\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

\chapter{INTRODUCTION}
\section{Background of the study}


In modern software systems, log files serve as critical sources of information
for system monitoring, debugging, troubleshooting, and security analysis. As applications or systems scale and increase its complexity,
the volume of generated log data gets bigger exponentially. Therefore, traditional manual approaches
for log analysis like manually examine through log files using basic tools such as grep or text
editors, has become less efficient and more defective \cite{author2023}. As the result,
it is becoming more difficult to detect anomalies within large scale system.

Over the year, a lot of automated log-based methods have been introduced to help detecting
system anomalies. These approaches usually require raw log preprocessing techniques, feature extraction
and machine-learning-based algorithms for processing vast amounts of
unstructured log data efficiently, identifying potential issues before they
escalate into critical failures. Recent advancements in natural language processing
and large language models (LLMs) have also increased the potential for
intelligent log analysis systems. However, despite these development, traditional machine-learning techniques and LLM-based approaches have yet to be efficiently integrated
into a single, cohesive log analysis framework.

This study focuses on leveraging existing machine learning approaches where log parsing and anomaly
detected are used, and augmenting them with large language model to provide  actionable insights and helpful recommendations.


\section{Problem Statement}

Despite the important role of log analysis that play in making the system more secure and reliable,
several significant challenges still persist in current practices:

\begin{itemize}
    \item \textit{Limited interpretability.} In log anomaly detection,
          the ability to interpret model's outputs is important for people who
          work as system administrators or analysts to effectively action to
          the alerts. They need to understand which log entries may be responsible
          for the detected abnormality. Yet, many traditional approaches only
          provide basic classified prediction without any explanation. As a result,
          engineers still have to perform further manual root cause analysis
          which in large-scale and complex systems becomes an very time-consuming
          and heavy task.

    \item \textit{Poor adaptability.} Many existing methods rely on a predefined
          set of log event templates during feature extraction phase (This phase will use the set of log event templates generated by
          log parsing to create numerical features for machine learning models \cite{SARHAN2024205}).
          However, as applications scale up and expand  in term of feature,
          new and unseen log events will definitely appear. Therefore, adapting to these
          changes require retraining models from scratch which make the systems become less practical
          in dynamic environments.

    \item \textit{Poor adaptability.}

\end{itemize}

\section{Objectives of the Study}

This study will perform the investigation on existing anomaly-detection
methodologies, compares the performance of different machine-learning
models, and develops a practical framework that integrates large language
models (LLMs) to automate and improve anomaly detection in real-world scenario.



\section{Limitations of the Study}


\chapter{LITERATURE REVIEW}

\section{Introduction to system log}

System logs, also known as event logs or audit trails, are automatically generated by
a computer system (its operating system, services, applications) that record events, changes and error that
occur inside that system. These log files serve an important role in monitoring system
health, diagnose failures, detecting anomalies, and ensure system security and compliance \cite{STUDIAWAN20191}.
System logs usually follow a semi-structured format which consist of several fields that capture
both the context and content of the event.\\[10pt]


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{hdfs-log.png}
    \caption{This is the descriptive text that explains the figure.}
    % \label{fig:my_image} % The label for cross-referencing
\end{figure}

Figure 2.1 show a log snippet that was generated by HDFS DataNode, one of the core storage components in the Hadoop ecosystem
(a more detail explanation of this system will be provided below).
These log entries are created from the storage operations of the system such as
data writes, packet handling, and communication between DataNodes. A log entry typically follow a structure as follow:

\begin{itemize}
    \item \textit{Timestamp: 2016-10-02 12:24:52,337.} Indicates the exact moment when the system processed the event.

    \item \textit{Log level: INFO.} Shows that the entry reports normal operational activity

    \item \textit{Log message: org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace:...} Log messages provide a structured description of system behavior,
          capturing events, performance indicators. This is the most important part of a log entries and therefor become the
          foundation for monitoring, auditing and anomaly detection.

\end{itemize}

Given their important role in providing such essential information about the system, logs must be reliably collected,
centrally stored and appropriately maintained  to enable accurate monitoring, troubleshooting, and subsequent analytical processes.

\section{Traditional log analysis method}

Traditional log analysis have relied on two primary approaches (cite something): manual log inspection and rule-based pattern matching systems.
While these techniques are the foundation of the system administrators fields and troubleshooting, it were used during the time
when the system complexity and log volumes were much smaller than nowadays modern systems. Understanding the strengths and limitations of
these approaches will provide you a better context and insights of logs which will enhance your log analysis systems.


\subsection{Manual Log Inspection}

Manual log inspection is one of the earliest and most straightforward methods used by system
administrators to identify issues. In this approach, developers or administrators will
directly operate searching on raw log files using command-line tools such as \verb|grep| , \verb|tail|, \verb|less|,
to locate error logs or correlate timestamp of the incidents. While feasible for small log volumes, this method is very
time-consuming, highly error-prone and relies heavily on an operator’s familiarity with the system. Therefore, this approach
is no longer a feasible solution for detecting anomalies in large-scale systems which may produce millions of log entries
per hour. For instance, in 2013 the Alibaba Cloud system was reported to generate approximately 100 to
200 million log lines in one hour \citep{6410318}.

\subsection{Rule-Based Systems (Regex and Pattern Matching)}

To counter the limitations of manual log review, developers created rule-based system that used fixed rules, regular expressions (regex),
and predefined patterns to filter, parse, and identify specific events when a specification conditions are met.
For example, system operators can create rules to detect keywords such as "ERROR", "FAILED" or extract key information
from log entries. Rule-based methods offer some advantages: they are easy to implement and effective for detecting
obvious or recurring errors. Many traditional monitoring tools such as Nagios, Splunk (early versions)
\cite{splunk_field_extraction}, and Logstash \cite{logstash_grok},
they are heavily depend on popular pattern-matching techniques to filter, categorize, and index log data.

However, the effectiveness of rule-based approaches highly depend on the completeness and accuracy of the
hand-crafted predefined rules. It will become more defective in a constantly evolving environments where logs
format change frequently and new types of anomalies will appear which do not match the existing patterns. Maintaining
large sets of regex rules is also a labor-intensive action and require continuous manual updates on patterns which is
unsuitable for adaptive anomaly detection.



\section{Log parsing}


\section{Machine learning and deep learning for log anomaly detection}

As system continuously scaled and logs volumes grew bigger, manual or rule-based approaches is
no longer available. As a result, researchers began to adopt machine learning to automate
anomaly detection. These methods are created to enable it to learn the structures, numerical representations,
and patterns of logs without depending on predefined rules or templates. These approaches are often categorized as
supervised or unsupervised learning.

\subsection{Supervised learning}

\begin{enumerate}[label=\alph*.] % Sets the label format to (a), (b), (c)...
    \item Support Vector Machine (SVM)

          Support Vector Machine is one of the fundamental methods that proposed
          by Vapnik \cite{vapnik1998statistical} which is used for binary classification.
          The basic idea of this method is as follow. For nonlinear separable data, we
          can transform it to higher-dimensional space where the data becomes linearly separable.
          This allows the classifier to construct an optimal separating hyperplane between normal
          and anomalous log patterns. In log detection context, anomaly logs will be treated as negatives
          and normal one will be served as positive examples. But using this method for detecting
          anomalies often often suffers from class imbalance, where the positive examples is outnumber
          the negatives. As the result, the classifier will overfit the anomalous logs, which make
          it become weak in detecting new occur, unseen logs.

          \begin{figure}[H]
              \centering
              \includegraphics[width=0.7\textwidth]{svm.jpg}
              \caption{SVM classification}
              % \label{fig:my_image} % The label for cross-referencing
          \end{figure}

    \item Decision Tree (DS)

          Decision tree is one of the earliest machine learning approaches that apply to
          anomaly log detection and are defined by Quinlan \cite{quinlan2014c4} as "powerful
          and common tools for classification and prediction". This method recursively
          splitting data based on its features then create a tree-like structure where
          each leaf node represents a prediction class (normal or abnormal).
          When apply to log anomaly detection, we have to transform log entries into
          structured numerical features before feed it into decision tree algorithm.
          The applications of decision trees to log-based anomaly detection has been
          proposed in many research with promising results. He et al. \cite{7774521}
          did some experiments with anomaly logs in HDFS dataset using decision tree classifier and
          and prove the effectiveness of it. The approach not only achieve high accuracy
          but also provide interpretable rules that allows others to validate and refine
          them as they wish.
          In context of this research, decision tree served as a foundation machine
          learning algorithm for detecting anomalies in log data. The specific implementations
          and results of using this algorithm will be discussed more upon in subsequent sections.

          % \begin{enumerate}[label=\arabic*.]
          %     \item The first item in the list.
          %     \item The second item in the list.
          %     \item The third item in the list.
          % \end{enumerate}



    \item The final item summarizes the topic.
\end{enumerate}

\subsection{Unsupervised learning}

In many situation when anomaly data is not labeled or unavailable, the supervised
learning is thereby become impractical. As a result, unsupervised learing methods
have been adopted widely for log anomaly detection. These approaches aim to analyze
data without requiring labeled examples by discovering the characteristics of the
data itseft. This section will examine some popular unsupervised approaches that are
currently applied to log analysis.

\begin{enumerate}[label=\alph*.] % Sets the label format to (a), (b), (c)...

    \item Isolation Forrest

          Isolation Forrest is a machine learning algorithm that was introduced by Liu et al
          \cite{liu2012isolation} in 2012. His core principle of isolation forrest is that
          anomolies are more accessible to isolate than abnomalous points \cite{Longberg2024}.
          The algorithms will construct forrest of random binary trees to separate each
          data point. Each tree us built by randomly selecting a feature from the dataset
          and a split value within the feature's range, then repetitive split the
          data until it reach the maximum depth or the instance is isolated.

          \begin{figure}[H]
              \centering
              \includegraphics[width=1\textwidth]{isolation-forrest.png}
              \caption{Isolation Forest}
              % \label{fig:my_image} % The label for cross-referencing
          \end{figure}

          Figure 2.3 demonstrates the isolation capabilities of Isolation Forest algorithm.
          In the left, the data point is seperated with only one split, indicating a high anomaly score.
          On the other hand, data point on the right requires more splits,  suggesting it’s a
          nominal data point
    \item Principal Component Analysis (PCA)

    \item Clustering

\end{enumerate}

- ml techniques: https://arxiv.org/pdf/2307.16714

\section{Large Language Models (LLMs) in Log Analysis}

Parsing with LLMs instead of templates:

% https://zeyang919.github.io/paper/LLMParser.pdf?utm_source=chatgpt.com

Explainability + anomaly detection:

\chapter{METHODOLOGIES}
This chapter presents the methodology used to develop and evaluate the proposed log anomaly detection framework, including data preparation, model design,
integration of large language models, and evaluation metrics.

\section{Overview}
\subsection{System Pipeline Overview}
assdas
\subsection{Research Approach}
fasdfasd

\section{System Architecture and Design}
\subsection{Overall Architecture}


\section{Dataset preparation}
\section{Log Parsing Methodology}


\section{Feature Extraction / Representation}
\section{Anomaly Detection Models}
\section{LLM Integration Method}
\section{Implementation Details}




\chapter{IMPLEMETATIONS}
\section{Implementation Overview}
\section{Backend Implementation}
\subsection{FastAPI Application Architecture}
- Code snippets from src/main.py
- Router organization
- Dependency injection
- Database session management

\subsection{Database Layer Implementation}
- model diagram
- detail

\subsection{API Endpoints Implementation}


\section{Log Processing Pipeline}
\subsection{File Upload and Storage}
- MinIO client integration
- File validation logic
- Unique filename generation

\subsection{Asynchronous Processing with Celery}
- Celery configuration
- DatabaseTask base class
- Task retry logic
- Job status updates

\subsection{Drain Parser Integration}
- Parser service wrapper
- Temporary file handling
- Template extraction
- Output structuring

\subsection{Feature Extraction}
- Event counting
- Statistical features
- CSV conversion

\subsection{Decision Tree Anomaly Detection}
- Model training code
- Feature preparation
- Prediction pipeline
- Result storage

\section{LLM Integration}
\subsection{LLM Service Design}
- API client setup
- Context preparation

\subsection{LLM Service Design}
- System prompts
- User prompts with context
- Few-shot examples (if any)

% Recommendation Generation
\subsection{Recommendation Generation}
- Anomaly result processing
- LLM API calls
- Response Parsing

\section{Frontend Implementation}

\subsection{React Application Structure}
- Component organization
- Routing setup

\subsection{Key Components}



\chapter{CONCLUSION AND RECOMMENDATION}










\bibliographystyle{plain}
\bibliography{references}
\end{document}